


PR Agency | Vector Inc. （Vector Group ）| Communication Expert




















vector



ENGLISH
繁体字
簡体字
Tiếng Việt
INDONESIA
ภาษาไทย

PR Agency | Vector Inc. （Vector Group ）
Good Resonance.

Vector’s Group
Vector’s Group
CEO Message
Asia Pacific Network
Services
Client
Contact us









Gives an overview of the company.








Describes Management Policy of the company.









Without action, you can’t achieve anything.And when it comes to choosing between acting first and thinking later, or thinking first and acting later,it's pretty clear what the answer is.Vector is a company that takes action!







Provides organization structure of vector group.












September 2016 :
Acquired LAUGH TECH Co., Ltd.


March 2016 :
PR TIMES Co., Ltd,a subsidiary, is listed on Tokyo Stock Exchange Mothers


June 2015 :
Established Video Wire Co., Ltd.(Currently NewsTV Co., Ltd.)


December 2014 :
Capital becomes1.7 billion yen after capital increase









PR Events : 
In China where many mass media exist and each city develops in different ways, it


Press Tours :
In China where domestic companies/brands tend to be appreciated, it is essential


PR with Mass Media :
We will select media candidate for exposure, draw up a press









Supplement manufacturer
Created base for product effectiveness awareness through TV tie-in.Conducted media


Eyewear brand
Created fan base at the official Weibo accountTransmitted teaser information








Explains the journey of vector group so far.







TOP


Contact us











Cross product - Wikipedia






















 






Cross product

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search

This article is about the cross product of two vectors in three-dimensional Euclidean space. For other uses, see Cross product (disambiguation).
In mathematics and vector algebra, the cross product or vector product (occasionally directed area product to emphasize the geometric significance) is a binary operation on two vectors in three-dimensional space (R3) and is denoted by the symbol ×. Given two linearly independent vectors a and b, the cross product, a × b, is a vector that is perpendicular to both a and b and thus normal to the plane containing them. It has many applications in mathematics, physics, engineering, and computer programming. It should not be confused with dot product (projection product).
If two vectors have the same direction (or have the exact opposite direction from one another, i.e. are not linearly independent) or if either one has zero length, then their cross product is zero. More generally, the magnitude of the product equals the area of a parallelogram with the vectors for sides; in particular, the magnitude of the product of two perpendicular vectors is the product of their lengths. The cross product is anticommutative (i.e., a × b = −(b × a)) and is distributive over addition (i.e., a × (b + c) = a × b + a × c). The space R3 together with the cross product is an algebra over the real numbers, which is neither commutative nor associative, but is a Lie algebra with the cross product being the Lie bracket.
Like the dot product, it depends on the metric of Euclidean space, but unlike the dot product, it also depends on a choice of orientation or "handedness". The product can be generalized in various ways; it can be made independent of orientation by changing the result to pseudovector, or in arbitrary dimensions the exterior product of vectors can be used with a bivector or two-form result. Also, using the orientation and metric structure just as for the traditional 3-dimensional cross product, one can in n dimensions take the product of n − 1 vectors to produce a vector perpendicular to all of them. But if the product is limited to non-trivial binary products with vector results, it exists only in three and seven dimensions.[1] If one adds the further requirement that the product be uniquely defined, then only the 3-dimensional cross product qualifies. (See § Generalizations, below, for other dimensions.)




The cross-product in respect to a right-handed coordinate system





Contents


1 Definition
2 Names
3 Computing the cross product

3.1 Coordinate notation
3.2 Matrix notation


4 Properties

4.1 Geometric meaning
4.2 Algebraic properties
4.3 Differentiation
4.4 Triple product expansion
4.5 Alternative formulation
4.6 Lagrange's identity
4.7 Infinitesimal generators of rotations


5 Alternative ways to compute the cross product

5.1 Conversion to matrix multiplication
5.2 Index notation for tensors
5.3 Mnemonic
5.4 Cross visualization


6 Applications

6.1 Computational geometry
6.2 Angular momentum and torque
6.3 Rigid body
6.4 Lorentz force
6.5 Other


7 Cross product as an exterior product
8 Cross product and handedness
9 Generalizations

9.1 Lie algebra
9.2 Quaternions
9.3 Octonions
9.4 Exterior product
9.5 Multilinear algebra
9.6 Skew-symmetric matrix


10 History
11 See also
12 Notes
13 References
14 External links



Definition[edit]




Finding the direction of the cross product by the right-hand rule.


The cross product of two vectors a and b is defined only in three-dimensional space and is denoted by a × b. In physics, sometimes the notation a ∧ b is used,[2] though this is avoided in mathematics to avoid confusion with the exterior product.
The cross product a × b is defined as a vector c that is perpendicular to both a and b, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span.
The cross product is defined by the formula[3][4]






a

×

b

=

∥

a

∥


∥

b

∥

sin
⁡
(
θ
)
 

n



{\displaystyle \mathbf {a} \times \mathbf {b} =\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|\sin(\theta )\ \mathbf {n} }



where θ is the angle between a and b in the plane containing them (hence, it is between 0° and 180°), ‖a‖ and ‖b‖ are the magnitudes of vectors a and b, and n is a unit vector perpendicular to the plane containing a and b in the direction given by the right-hand rule (illustrated). If the vectors a and b are parallel (i.e., the angle θ between them is either 0° or 180°), by the above formula, the cross product of a and b is the zero vector 0.




The cross product a × b (vertical, in purple) changes as the angle between the vectors a (blue) and b (red) changes. The cross product is always orthogonal to both vectors, and has magnitude zero when the vectors are parallel and maximum magnitude ‖a‖‖b‖ when they are orthogonal.


By convention, the direction of the vector n is given by the right-hand rule, where one simply points the forefinger of the right hand in the direction of a and the middle finger in the direction of b. Then, the vector n is coming out of the thumb (see the picture on the right). Using this rule implies that the cross-product is anti-commutative, i.e., b × a = −(a × b). By pointing the forefinger toward b first, and then pointing the middle finger toward a, the thumb will be forced in the opposite direction, reversing the sign of the product vector.
Using the cross product requires the handedness of the coordinate system to be taken into account (as explicit in the definition above). If a left-handed coordinate system is used, the direction of the vector n is given by the left-hand rule and points in the opposite direction.
This, however, creates a problem because transforming from one arbitrary reference system to another (e.g., a mirror image transformation from a right-handed to a left-handed coordinate system), should not change the direction of n. The problem is clarified by realizing that the cross product of two vectors is not a (true) vector, but rather a pseudovector. See cross product and handedness for more detail.
Names[edit]




According to Sarrus's rule, the determinant of a 3×3 matrix involves multiplications between matrix elements identified by crossed diagonals


In 1881, Josiah Willard Gibbs, and independently Oliver Heaviside, introduced both the dot product and the cross product using a period (a . b) and an "x" (a x b), respectively, to denote them.[5]
In 1877, to emphasize the fact that the result of a dot product is a scalar while the result of a cross product is a vector, William Kingdon Clifford coined the alternative names scalar product and vector product for the two operations.[5] These alternative names are still widely used in the literature.
Both the cross notation (a × b) and the name cross product were possibly inspired by the fact that each scalar component of a × b is computed by multiplying non-corresponding components of a and b. Conversely, a dot product a ⋅ b involves multiplications between corresponding components of a and b. As explained below, the cross product can be expressed in the form of a determinant of a special 3 × 3 matrix. According to Sarrus's rule, this involves multiplications between matrix elements identified by crossed diagonals.
Computing the cross product[edit]
Coordinate notation[edit]




Standard basis vectors (i, j, k, also denoted e1, e2, e3) and vector components of a (ax, ay, az, also denoted a1, a2, a3)


The standard basis vectors i, j, and k satisfy the following equalities in a right hand coordinate system:










i

×

j




=

k






j

×

k




=

i






k

×

i




=

j







{\displaystyle {\begin{aligned}\mathbf {i} \times \mathbf {j} &=\mathbf {k} \\\mathbf {j} \times \mathbf {k} &=\mathbf {i} \\\mathbf {k} \times \mathbf {i} &=\mathbf {j} \end{aligned}}}



which imply, by the anticommutativity of the cross product, that










j
×
i




=
−

k






k
×
j




=
−

i






i
×
k




=
−

j







{\displaystyle {\begin{aligned}\mathbf {j\times i} &=-\mathbf {k} \\\mathbf {k\times j} &=-\mathbf {i} \\\mathbf {i\times k} &=-\mathbf {j} \end{aligned}}}



The definition of the cross product also implies that






i

×

i

=

j

×

j

=

k

×

k

=

0



{\displaystyle \mathbf {i} \times \mathbf {i} =\mathbf {j} \times \mathbf {j} =\mathbf {k} \times \mathbf {k} =\mathbf {0} }

 (the zero vector).

These equalities, together with the distributivity and linearity of the cross product (but both do not follow easily from the definition given above), are sufficient to determine the cross product of any two vectors u and v. Each vector can be defined as the sum of three orthogonal components parallel to the standard basis vectors:










u




=

u

1



i

+

u

2



j

+

u

3



k






v




=

v

1



i

+

v

2



j

+

v

3



k







{\displaystyle {\begin{aligned}\mathbf {u} &=u_{1}\mathbf {i} +u_{2}\mathbf {j} +u_{3}\mathbf {k} \\\mathbf {v} &=v_{1}\mathbf {i} +v_{2}\mathbf {j} +v_{3}\mathbf {k} \end{aligned}}}



Their cross product u × v can be expanded using distributivity:










u

×

v

=





(

u

1



i

+

u

2



j

+

u

3



k

)
×
(

v

1



i

+

v

2



j

+

v

3



k

)




=





u

1



v

1


(

i

×

i

)
+

u

1



v

2


(

i

×

j

)
+

u

1



v

3


(

i

×

k

)
+








u

2



v

1


(

j

×

i

)
+

u

2



v

2


(

j

×

j

)
+

u

2



v

3


(

j

×

k

)
+








u

3



v

1


(

k

×

i

)
+

u

3



v

2


(

k

×

j

)
+

u

3



v

3


(

k

×

k

)






{\displaystyle {\begin{aligned}\mathbf {u} \times \mathbf {v} ={}&(u_{1}\mathbf {i} +u_{2}\mathbf {j} +u_{3}\mathbf {k} )\times (v_{1}\mathbf {i} +v_{2}\mathbf {j} +v_{3}\mathbf {k} )\\={}&u_{1}v_{1}(\mathbf {i} \times \mathbf {i} )+u_{1}v_{2}(\mathbf {i} \times \mathbf {j} )+u_{1}v_{3}(\mathbf {i} \times \mathbf {k} )+{}\\&u_{2}v_{1}(\mathbf {j} \times \mathbf {i} )+u_{2}v_{2}(\mathbf {j} \times \mathbf {j} )+u_{2}v_{3}(\mathbf {j} \times \mathbf {k} )+{}\\&u_{3}v_{1}(\mathbf {k} \times \mathbf {i} )+u_{3}v_{2}(\mathbf {k} \times \mathbf {j} )+u_{3}v_{3}(\mathbf {k} \times \mathbf {k} )\\\end{aligned}}}



This can be interpreted as the decomposition of u × v into the sum of nine simpler cross products involving vectors aligned with i, j, or k. Each one of these nine cross products operates on two vectors that are easy to handle as they are either parallel or orthogonal to each other. From this decomposition, by using the above-mentioned equalities and collecting similar terms, we obtain:










u

×

v

=





−

u

1



v

1



0

+

u

1



v

2



k

−

u

1



v

3



j







−

u

2



v

1



k

−

u

2



v

2



0

+

u

2



v

3



i







+

u

3



v

1



j

−

u

3



v

2



i

−

u

3



v

3



0





=





(

u

2



v

3


−

u

3



v

2


)

i

+
(

u

3



v

1


−

u

1



v

3


)

j

+
(

u

1



v

2


−

u

2



v

1


)

k







{\displaystyle {\begin{aligned}\mathbf {u} \times \mathbf {v} ={}&-u_{1}v_{1}\mathbf {0} +u_{1}v_{2}\mathbf {k} -u_{1}v_{3}\mathbf {j} \\&-u_{2}v_{1}\mathbf {k} -u_{2}v_{2}\mathbf {0} +u_{2}v_{3}\mathbf {i} \\&+u_{3}v_{1}\mathbf {j} -u_{3}v_{2}\mathbf {i} -u_{3}v_{3}\mathbf {0} \\={}&(u_{2}v_{3}-u_{3}v_{2})\mathbf {i} +(u_{3}v_{1}-u_{1}v_{3})\mathbf {j} +(u_{1}v_{2}-u_{2}v_{1})\mathbf {k} \\\end{aligned}}}



meaning that the three scalar components of the resulting vector s = s1i + s2j + s3k = u × v are










s

1





=

u

2



v

3


−

u

3



v

2







s

2





=

u

3



v

1


−

u

1



v

3







s

3





=

u

1



v

2


−

u

2



v

1








{\displaystyle {\begin{aligned}s_{1}&=u_{2}v_{3}-u_{3}v_{2}\\s_{2}&=u_{3}v_{1}-u_{1}v_{3}\\s_{3}&=u_{1}v_{2}-u_{2}v_{1}\end{aligned}}}



Using column vectors, we can represent the same result as follows:







(




s

1







s

2







s

3





)


=


(




u

2



v

3


−

u

3



v

2







u

3



v

1


−

u

1



v

3







u

1



v

2


−

u

2



v

1





)




{\displaystyle {\begin{pmatrix}s_{1}\\s_{2}\\s_{3}\end{pmatrix}}={\begin{pmatrix}u_{2}v_{3}-u_{3}v_{2}\\u_{3}v_{1}-u_{1}v_{3}\\u_{1}v_{2}-u_{2}v_{1}\end{pmatrix}}}



Matrix notation[edit]




Use of Sarrus's rule to find the cross product of u and v


The cross product can also be expressed as the formal[note 1] determinant:






u
×
v

=


|




i




j




k






u

1





u

2





u

3







v

1





v

2





v

3





|




{\displaystyle \mathbf {u\times v} ={\begin{vmatrix}\mathbf {i} &\mathbf {j} &\mathbf {k} \\u_{1}&u_{2}&u_{3}\\v_{1}&v_{2}&v_{3}\\\end{vmatrix}}}



This determinant can be computed using Sarrus's rule or cofactor expansion. Using Sarrus's rule, it expands to










u
×
v




=
(

u

2



v

3



i

+

u

3



v

1



j

+

u

1



v

2



k

)
−
(

u

3



v

2



i

+

u

1



v

3



j

+

u

2



v

1



k

)






=
(

u

2



v

3


−

u

3



v

2


)

i

+
(

u

3



v

1


−

u

1



v

3


)

j

+
(

u

1



v

2


−

u

2



v

1


)

k

.






{\displaystyle {\begin{aligned}\mathbf {u\times v} &=(u_{2}v_{3}\mathbf {i} +u_{3}v_{1}\mathbf {j} +u_{1}v_{2}\mathbf {k} )-(u_{3}v_{2}\mathbf {i} +u_{1}v_{3}\mathbf {j} +u_{2}v_{1}\mathbf {k} )\\&=(u_{2}v_{3}-u_{3}v_{2})\mathbf {i} +(u_{3}v_{1}-u_{1}v_{3})\mathbf {j} +(u_{1}v_{2}-u_{2}v_{1})\mathbf {k} .\end{aligned}}}



Using cofactor expansion along the first row instead, it expands to[6]






u
×
v

=


|




u

2





u

3







v

2





v

3





|



i

−


|




u

1





u

3







v

1





v

3





|



j

+


|




u

1





u

2







v

1





v

2





|



k



{\displaystyle \mathbf {u\times v} ={\begin{vmatrix}u_{2}&u_{3}\\v_{2}&v_{3}\end{vmatrix}}\mathbf {i} -{\begin{vmatrix}u_{1}&u_{3}\\v_{1}&v_{3}\end{vmatrix}}\mathbf {j} +{\begin{vmatrix}u_{1}&u_{2}\\v_{1}&v_{2}\end{vmatrix}}\mathbf {k} }



which gives the components of the resulting vector directly.
Properties[edit]
Geometric meaning[edit]
See also: Triple product




Figure 1. The area of a parallelogram as the magnitude of a cross product






Figure 2. Three vectors defining a parallelepiped


The magnitude of the cross product can be interpreted as the positive area of the parallelogram having a and b as sides (see Figure 1):






∥

a

×

b

∥

=

∥

a

∥


∥

b

∥

sin
⁡
θ
.


{\displaystyle \left\|\mathbf {a} \times \mathbf {b} \right\|=\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|\sin \theta .}



Indeed, one can also compute the volume V of a parallelepiped having a, b and c as edges by using a combination of a cross product and a dot product, called scalar triple product (see Figure 2):






a

⋅
(

b

×

c

)
=

b

⋅
(

c

×

a

)
=

c

⋅
(

a

×

b

)
.


{\displaystyle \mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} )=\mathbf {b} \cdot (\mathbf {c} \times \mathbf {a} )=\mathbf {c} \cdot (\mathbf {a} \times \mathbf {b} ).}



Since the result of the scalar triple product may be negative, the volume of the parallelepiped is given by its absolute value. For instance,





V
=

|


a

⋅
(

b

×

c

)

|

.


{\displaystyle V=|\mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} )|.}



Because the magnitude of the cross product goes by the sine of the angle between its arguments, the cross product can be thought of as a measure of perpendicularity in the same way that the dot product is a measure of parallelism. Given two unit vectors, their cross product has a magnitude of 1 if the two are perpendicular and a magnitude of zero if the two are parallel. The dot product of two unit vectors behaves just oppositely: it is zero when the unit vectors are perpendicular and 1 if the unit vectors are parallel.
Unit vectors enable two convenient identities: the dot product of two unit vectors yields the cosine (which may be positive or negative) of the angle between the two unit vectors. The magnitude of the cross product of the two unit vectors yields the sine (which will always be positive).
Algebraic properties[edit]




Cross product scalar multiplication. Left: Decomposition of b into components parallel and perpendicular to a. Right: Scaling of the perpendicular components by a positive real number r (if negative, b and the cross product are reversed).






Cross product distributivity over vector addition. Left: The vectors b and c are resolved into parallel and perpendicular components to a. Right: The parallel components vanish in the cross product, only the perpendicular components shown in the plane perpendicular to a remain.[7]






The two nonequivalent triple cross products of three vectors a, b, c. In each case, two vectors define a plane, the other is out of the plane and can be split into parallel and perpendicular components to the cross product of the vectors defining the plane. These components can be found by vector projection and rejection. The triple product is in the plane and is rotated as shown.


If the cross product of two vectors is the zero vector (i.e. a × b = 0), then either one or both of the inputs is the zero vector, (a = 0 or b = 0) or else they are parallel or antiparallel (a ∥ b) so that the sine of the angle between them is zero (θ = 0° or θ = 180° and sinθ = 0).
The self cross product of a vector is the zero vector:






a

×

a

=

0



{\displaystyle \mathbf {a} \times \mathbf {a} =\mathbf {0} }



The cross product is anticommutative,






a

×

b

=
−
(

b

×

a

)
,


{\displaystyle \mathbf {a} \times \mathbf {b} =-(\mathbf {b} \times \mathbf {a} ),}



distributive over addition,






a

×
(

b

+

c

)
=
(

a

×

b

)
+
(

a

×

c

)
,


{\displaystyle \mathbf {a} \times (\mathbf {b} +\mathbf {c} )=(\mathbf {a} \times \mathbf {b} )+(\mathbf {a} \times \mathbf {c} ),}



and compatible with scalar multiplication so that





(
r

a

)
×

b

=

a

×
(
r

b

)
=
r
(

a

×

b

)
.


{\displaystyle (r\mathbf {a} )\times \mathbf {b} =\mathbf {a} \times (r\mathbf {b} )=r(\mathbf {a} \times \mathbf {b} ).}



It is not associative, but satisfies the Jacobi identity:






a

×
(

b

×

c

)
+

b

×
(

c

×

a

)
+

c

×
(

a

×

b

)
=

0

.


{\displaystyle \mathbf {a} \times (\mathbf {b} \times \mathbf {c} )+\mathbf {b} \times (\mathbf {c} \times \mathbf {a} )+\mathbf {c} \times (\mathbf {a} \times \mathbf {b} )=\mathbf {0} .}



Distributivity, linearity and Jacobi identity show that the R3 vector space together with vector addition and the cross product forms a Lie algebra, the Lie algebra of the real orthogonal group in 3 dimensions, SO(3). The cross product does not obey the cancellation law: that is, a × b = a × c with a ≠ 0 does not imply b = c, but only that:










0




=
(

a

×

b

)
−
(

a

×

c

)






=

a

×
(

b

−

c

)
.






{\displaystyle {\begin{aligned}\mathbf {0} &=(\mathbf {a} \times \mathbf {b} )-(\mathbf {a} \times \mathbf {c} )\\&=\mathbf {a} \times (\mathbf {b} -\mathbf {c} ).\\\end{aligned}}}



From the definition of the cross product, the angle between a and b − c must be zero, and these vectors must be parallel. That is, they are related by a scale factor t, leading to:






c

=

b

+
t

a

,


{\displaystyle \mathbf {c} =\mathbf {b} +t\mathbf {a} ,}



for some scalar t.
If a ⋅ b = a ⋅ c and a × b = a × c, for non-zero vector a, then b = c, as






a

×
(

b

−

c

)
=

0



{\displaystyle \mathbf {a} \times (\mathbf {b} -\mathbf {c} )=\mathbf {0} }



and






a

⋅
(

b

−

c

)
=
0
,


{\displaystyle \mathbf {a} \cdot (\mathbf {b} -\mathbf {c} )=0,}



so b − c is both parallel and perpendicular to the non-zero vector a, something that is only possible if b − c = 0 so they are identical.
From the geometrical definition, the cross product is invariant under proper rotations about the axis defined by a × b. In formulae:





(
R

a

)
×
(
R

b

)
=
R
(

a

×

b

)


{\displaystyle (R\mathbf {a} )\times (R\mathbf {b} )=R(\mathbf {a} \times \mathbf {b} )}

, where 



R


{\displaystyle R}

 is a rotation matrix with 



det
(
R
)
=
1


{\displaystyle \det(R)=1}

.

More generally, the cross product obeys the following identity under matrix transformations:





(
M

a

)
×
(
M

b

)
=
(
det
M
)
(

M

−
1



)


T



(

a

×

b

)
=
cof
⁡
M
(

a

×

b

)


{\displaystyle (M\mathbf {a} )\times (M\mathbf {b} )=(\det M)(M^{-1})^{\mathrm {T} }(\mathbf {a} \times \mathbf {b} )=\operatorname {cof} M(\mathbf {a} \times \mathbf {b} )}



where 



M


{\displaystyle M}

 is a 3-by-3 matrix and 



(

M

−
1



)


T





{\displaystyle (M^{-1})^{\mathrm {T} }}

 is the transpose of the inverse and 



cof


{\displaystyle \operatorname {cof} }

 is the cofactor matrix. It can be readily seen how this formula reduces to the former one if 



M


{\displaystyle M}

 is a rotation matrix.
The cross product of two vectors lies in the null space of the 2 × 3 matrix with the vectors as rows:






a

×

b

∈
N
S

(


[




a






b




]


)

.


{\displaystyle \mathbf {a} \times \mathbf {b} \in NS\left({\begin{bmatrix}\mathbf {a} \\\mathbf {b} \end{bmatrix}}\right).}



For the sum of two cross products, the following identity holds:






a

×

b

+

c

×

d

=
(

a

−

c

)
×
(

b

−

d

)
+

a

×

d

+

c

×

b

.


{\displaystyle \mathbf {a} \times \mathbf {b} +\mathbf {c} \times \mathbf {d} =(\mathbf {a} -\mathbf {c} )\times (\mathbf {b} -\mathbf {d} )+\mathbf {a} \times \mathbf {d} +\mathbf {c} \times \mathbf {b} .}



Differentiation[edit]
Main article: Vector-valued function § Derivative and vector multiplication
The product rule of differential calculus applies to any bilinear operation, and therefore also to the cross product:







d

d
t



(

a

×

b

)
=



d

a



d
t



×

b

+

a

×



d

b



d
t



,


{\displaystyle {\frac {d}{dt}}(\mathbf {a} \times \mathbf {b} )={\frac {d\mathbf {a} }{dt}}\times \mathbf {b} +\mathbf {a} \times {\frac {d\mathbf {b} }{dt}},}



where a and b are vectors that depend on the real variable t.
Triple product expansion[edit]
Main article: Triple product
The cross product is used in both forms of the triple product. The scalar triple product of three vectors is defined as






a

⋅
(

b

×

c

)
,


{\displaystyle \mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} ),}



It is the signed volume of the parallelepiped with edges a, b and c and as such the vectors can be used in any order that's an even permutation of the above ordering. The following therefore are equal:






a

⋅
(

b

×

c

)
=

b

⋅
(

c

×

a

)
=

c

⋅
(

a

×

b

)
,


{\displaystyle \mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} )=\mathbf {b} \cdot (\mathbf {c} \times \mathbf {a} )=\mathbf {c} \cdot (\mathbf {a} \times \mathbf {b} ),}



The vector triple product is the cross product of a vector with the result of another cross product, and is related to the dot product by the following formula






a

×
(

b

×

c

)
=

b

(

a

⋅

c

)
−

c

(

a

⋅

b

)
.


{\displaystyle \mathbf {a} \times (\mathbf {b} \times \mathbf {c} )=\mathbf {b} (\mathbf {a} \cdot \mathbf {c} )-\mathbf {c} (\mathbf {a} \cdot \mathbf {b} ).}



The mnemonic "BAC minus CAB" is used to remember the order of the vectors in the right hand member. This formula is used in physics to simplify vector calculations. A special case, regarding gradients and useful in vector calculus, is









∇
×
(
∇
×

f

)



=
∇
(
∇
⋅

f

)
−
(
∇
⋅
∇
)

f







=
∇
(
∇
⋅

f

)
−

∇

2



f

,






{\displaystyle {\begin{aligned}\nabla \times (\nabla \times \mathbf {f} )&=\nabla (\nabla \cdot \mathbf {f} )-(\nabla \cdot \nabla )\mathbf {f} \\&=\nabla (\nabla \cdot \mathbf {f} )-\nabla ^{2}\mathbf {f} ,\\\end{aligned}}}



where ∇2 is the vector Laplacian operator.
Other identities relate the cross product to the scalar triple product:









(

a

×

b

)
×
(

a

×

c

)



=
(

a

⋅
(

b

×

c

)
)

a





(

a

×

b

)
⋅
(

c

×

d

)



=


b



T




(

(


c



T




a

)

I
−

c



a



T



)


d







=
(

a

⋅

c

)
(

b

⋅

d

)
−
(

a

⋅

d

)
(

b

⋅

c

)






{\displaystyle {\begin{aligned}(\mathbf {a} \times \mathbf {b} )\times (\mathbf {a} \times \mathbf {c} )&=(\mathbf {a} \cdot (\mathbf {b} \times \mathbf {c} ))\mathbf {a} \\(\mathbf {a} \times \mathbf {b} )\cdot (\mathbf {c} \times \mathbf {d} )&=\mathbf {b} ^{\mathrm {T} }\left(\left(\mathbf {c} ^{\mathrm {T} }\mathbf {a} \right)I-\mathbf {c} \mathbf {a} ^{\mathrm {T} }\right)\mathbf {d} \\&=(\mathbf {a} \cdot \mathbf {c} )(\mathbf {b} \cdot \mathbf {d} )-(\mathbf {a} \cdot \mathbf {d} )(\mathbf {b} \cdot \mathbf {c} )\end{aligned}}}



where I is the identity matrix.
Alternative formulation[edit]
The cross product and the dot product are related by:







∥

a

×

b

∥


2


=


∥

a

∥


2




∥

b

∥


2


−
(

a

⋅

b


)

2


.


{\displaystyle \left\|\mathbf {a} \times \mathbf {b} \right\|^{2}=\left\|\mathbf {a} \right\|^{2}\left\|\mathbf {b} \right\|^{2}-(\mathbf {a} \cdot \mathbf {b} )^{2}.}



The right-hand side is the Gram determinant of a and b, the square of the area of the parallelogram defined by the vectors. This condition determines the magnitude of the cross product. Namely, since the dot product is defined, in terms of the angle θ between the two vectors, as:






a
⋅
b

=

∥

a

∥


∥

b

∥

cos
⁡
θ
,


{\displaystyle \mathbf {a\cdot b} =\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|\cos \theta ,}



the above given relationship can be rewritten as follows:







∥

a
×
b

∥


2


=


∥

a

∥


2




∥

b

∥


2



(
1
−

cos

2


⁡
θ
)

.


{\displaystyle \left\|\mathbf {a\times b} \right\|^{2}=\left\|\mathbf {a} \right\|^{2}\left\|\mathbf {b} \right\|^{2}\left(1-\cos ^{2}\theta \right).}



Invoking the Pythagorean trigonometric identity one obtains:






∥

a

×

b

∥

=

∥

a

∥


∥

b

∥


|
sin
⁡
θ
|

,


{\displaystyle \left\|\mathbf {a} \times \mathbf {b} \right\|=\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|\left|\sin \theta \right|,}



which is the magnitude of the cross product expressed in terms of θ, equal to the area of the parallelogram defined by a and b (see definition above).
The combination of this requirement and the property that the cross product be orthogonal to its constituents a and b provides an alternative definition of the cross product.[8]
Lagrange's identity[edit]
The relation:







∥

a

×

b

∥


2


=
det


[




a

⋅

a




a

⋅

b






a

⋅

b




b

⋅

b




]


=


∥

a

∥


2




∥

b

∥


2


−
(

a

⋅

b


)

2


.


{\displaystyle \left\|\mathbf {a} \times \mathbf {b} \right\|^{2}=\det {\begin{bmatrix}\mathbf {a} \cdot \mathbf {a} &\mathbf {a} \cdot \mathbf {b} \\\mathbf {a} \cdot \mathbf {b} &\mathbf {b} \cdot \mathbf {b} \\\end{bmatrix}}=\left\|\mathbf {a} \right\|^{2}\left\|\mathbf {b} \right\|^{2}-(\mathbf {a} \cdot \mathbf {b} )^{2}.}



can be compared with another relation involving the right-hand side, namely Lagrange's identity expressed as:[9]






∑

1
≤
i
<
j
≤
n




(

a

i



b

j


−

a

j



b

i


)


2


=


∥

a

∥


2




∥

b

∥


2


−
(

a
⋅
b


)

2


 
,


{\displaystyle \sum _{1\leq i<j\leq n}\left(a_{i}b_{j}-a_{j}b_{i}\right)^{2}=\left\|\mathbf {a} \right\|^{2}\left\|\mathbf {b} \right\|^{2}-(\mathbf {a\cdot b} )^{2}\ ,}



where a and b may be n-dimensional vectors. This also shows that the Riemannian volume form for surfaces is exactly the surface element from vector calculus. In the case where n = 3, combining these two equations results in the expression for the magnitude of the cross product in terms of its components:[10]











|

a

×

b

|


2


=

∑

1
≤
i
<
j
≤
3




(

a

i



b

j


−

a

j



b

i


)


2


=




(

a

1



b

2


−

b

1



a

2



)

2


+
(

a

2



b

3


−

a

3



b

2



)

2


+
(

a

3



b

1


−

a

1



b

3



)

2


 
.






{\displaystyle {\begin{aligned}\left\vert \mathbf {a} \times \mathbf {b} \right\vert ^{2}=\sum _{1\leq i<j\leq 3}\left(a_{i}b_{j}-a_{j}b_{i}\right)^{2}=\\(a_{1}b_{2}-b_{1}a_{2})^{2}+(a_{2}b_{3}-a_{3}b_{2})^{2}+(a_{3}b_{1}-a_{1}b_{3})^{2}\ .\end{aligned}}}



The same result is found directly using the components of the cross-product found from:






a

×

b

=
det


[




i




j




k






a

1





a

2





a

3







b

1





b

2





b

3





]


.


{\displaystyle \mathbf {a} \times \mathbf {b} =\det {\begin{bmatrix}\mathbf {i} &\mathbf {j} &\mathbf {k} \\a_{1}&a_{2}&a_{3}\\b_{1}&b_{2}&b_{3}\\\end{bmatrix}}.}



In R3, Lagrange's equation is a special case of the multiplicativity | vw | = | v || w | of the norm in the quaternion algebra.
It is a special case of another formula, also sometimes called Lagrange's identity, which is the three dimensional case of the Binet-Cauchy identity:[11][12]





(

a

×

b

)
⋅
(

c

×

d

)
=
(

a

⋅

c

)
(

b

⋅

d

)
−
(

a

⋅

d

)
(

b

⋅

c

)
.


{\displaystyle (\mathbf {a} \times \mathbf {b} )\cdot (\mathbf {c} \times \mathbf {d} )=(\mathbf {a} \cdot \mathbf {c} )(\mathbf {b} \cdot \mathbf {d} )-(\mathbf {a} \cdot \mathbf {d} )(\mathbf {b} \cdot \mathbf {c} ).}



If a = c and b = d this simplifies to the formula above.
Infinitesimal generators of rotations[edit]
The cross product conveniently describes the infinitesimal generators of rotations in R3. Specifically, if n is a unit vector in R3 and R(φ, n) denotes a rotation about the axis through the origin specified by n, with angle φ (measured in radians, counterclockwise when viewed from the tip of n), then










d

d
ϕ



|


ϕ
=
0


R
(
ϕ
,

n

)

x

=

n

×

x



{\displaystyle \left.{d \over d\phi }\right|_{\phi =0}R(\phi ,{\boldsymbol {n}}){\boldsymbol {x}}={\boldsymbol {n}}\times {\boldsymbol {x}}}



for every vector x in R3. The cross product with n therefore describes the infinitesimal generator of the rotations about n. These infinitesimal generators form the Lie algebra so(3) of the rotation group SO(3), and we obtain the result that the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3).
Alternative ways to compute the cross product[edit]
Conversion to matrix multiplication[edit]
The vector cross product also can be expressed as the product of a skew-symmetric matrix and a vector:[11]






a

×

b

=
[

a


]

×



b

=


[




0



−

a

3







a

2









a

3




0



−

a

1






−

a

2







a

1





0



]




[




b

1







b

2







b

3





]




{\displaystyle \mathbf {a} \times \mathbf {b} =[\mathbf {a} ]_{\times }\mathbf {b} ={\begin{bmatrix}\,0&\!-a_{3}&\,\,a_{2}\\\,\,a_{3}&0&\!-a_{1}\\-a_{2}&\,\,a_{1}&\,0\end{bmatrix}}{\begin{bmatrix}b_{1}\\b_{2}\\b_{3}\end{bmatrix}}}







a

×

b

=
[

b


]

×



T




a

=


[




0





b

3





−

b

2






−

b

3




0





b

1









b

2





−

b

1





0



]




[




a

1







a

2







a

3





]


,


{\displaystyle \mathbf {a} \times \mathbf {b} =[\mathbf {b} ]_{\times }^{\mathrm {T} }\mathbf {a} ={\begin{bmatrix}\,0&\,\,b_{3}&\!-b_{2}\\-b_{3}&0&\,\,b_{1}\\\,\,b_{2}&\!-b_{1}&\,0\end{bmatrix}}{\begin{bmatrix}a_{1}\\a_{2}\\a_{3}\end{bmatrix}},}



where superscript T refers to the transpose operation, and [a]× is defined by:





[

a


]

×






=



d
e
f







[





0



−

a

3








a

2










a

3




0



−

a

1







−

a

2







a

1






0



]


.


{\displaystyle [\mathbf {a} ]_{\times }{\stackrel {\rm {def}}{=}}{\begin{bmatrix}\,\,0&\!-a_{3}&\,\,\,a_{2}\\\,\,\,a_{3}&0&\!-a_{1}\\\!-a_{2}&\,\,a_{1}&\,\,0\end{bmatrix}}.}



The columns [a]×,i of the skew-symmetric matrix for a vector a can be also obtained by calculating the cross-product with unit vectors, i.e.:





[

a


]

×
,
i


=

a

×





e
^




i



,

i
∈
{
1
,
2
,
3
}


{\displaystyle [\mathbf {a} ]_{\times ,i}=\mathbf {a} \times \mathbf {{\hat {e}}_{i}} ,\;i\in \{1,2,3\}}



Also, if a is itself expressed as a cross product:






a

=

c

×

d



{\displaystyle \mathbf {a} =\mathbf {c} \times \mathbf {d} }



then





[

a


]

×


=

d



c



T



−

c



d



T



.


{\displaystyle [\mathbf {a} ]_{\times }=\mathbf {d} \mathbf {c} ^{\mathrm {T} }-\mathbf {c} \mathbf {d} ^{\mathrm {T} }.}







Proof by substitution


Evaluation of the cross product gives






a

=

c

×

d

=


(




c

2



d

3


−

c

3



d

2







c

3



d

1


−

c

1



d

3







c

1



d

2


−

c

2



d

1





)




{\displaystyle \mathbf {a} =\mathbf {c} \times \mathbf {d} ={\begin{pmatrix}c_{2}d_{3}-c_{3}d_{2}\\c_{3}d_{1}-c_{1}d_{3}\\c_{1}d_{2}-c_{2}d_{1}\end{pmatrix}}}



Hence, the left hand side equals





[

a


]

×


=


[



0



c

2



d

1


−

c

1



d

2





c

3



d

1


−

c

1



d

3







c

1



d

2


−

c

2



d

1




0



c

3



d

2


−

c

2



d

3







c

1



d

3


−

c

3



d

1





c

2



d

3


−

c

3



d

2




0



]




{\displaystyle [\mathbf {a} ]_{\times }={\begin{bmatrix}0&c_{2}d_{1}-c_{1}d_{2}&c_{3}d_{1}-c_{1}d_{3}\\c_{1}d_{2}-c_{2}d_{1}&0&c_{3}d_{2}-c_{2}d_{3}\\c_{1}d_{3}-c_{3}d_{1}&c_{2}d_{3}-c_{3}d_{2}&0\end{bmatrix}}}



Now, for the right hand side,






c



d



T



=


[




c

1



d

1





c

1



d

2





c

1



d

3







c

2



d

1





c

2



d

2





c

2



d

3







c

3



d

1





c

3



d

2





c

3



d

3





]




{\displaystyle \mathbf {c} \mathbf {d} ^{\mathrm {T} }={\begin{bmatrix}c_{1}d_{1}&c_{1}d_{2}&c_{1}d_{3}\\c_{2}d_{1}&c_{2}d_{2}&c_{2}d_{3}\\c_{3}d_{1}&c_{3}d_{2}&c_{3}d_{3}\end{bmatrix}}}



And its transpose is






d



c



T



=


[




c

1



d

1





c

2



d

1





c

3



d

1







c

1



d

2





c

2



d

2





c

3



d

2







c

1



d

3





c

2



d

3





c

3



d

3





]




{\displaystyle \mathbf {d} \mathbf {c} ^{\mathrm {T} }={\begin{bmatrix}c_{1}d_{1}&c_{2}d_{1}&c_{3}d_{1}\\c_{1}d_{2}&c_{2}d_{2}&c_{3}d_{2}\\c_{1}d_{3}&c_{2}d_{3}&c_{3}d_{3}\end{bmatrix}}}



Evaluation of the right hand side gives






d



c



T



−

c



d



T



=


[



0



c

2



d

1


−

c

1



d

2





c

3



d

1


−

c

1



d

3







c

1



d

2


−

c

2



d

1




0



c

3



d

2


−

c

2



d

3







c

1



d

3


−

c

3



d

1





c

2



d

3


−

c

3



d

2




0



]




{\displaystyle \mathbf {d} \mathbf {c} ^{\mathrm {T} }-\mathbf {c} \mathbf {d} ^{\mathrm {T} }={\begin{bmatrix}0&c_{2}d_{1}-c_{1}d_{2}&c_{3}d_{1}-c_{1}d_{3}\\c_{1}d_{2}-c_{2}d_{1}&0&c_{3}d_{2}-c_{2}d_{3}\\c_{1}d_{3}-c_{3}d_{1}&c_{2}d_{3}-c_{3}d_{2}&0\end{bmatrix}}}



Comparison shows that the left hand side equals the right hand side.





This result can be generalized to higher dimensions using geometric algebra. In particular in any dimension bivectors can be identified with skew-symmetric matrices, so the product between a skew-symmetric matrix and vector is equivalent to the grade-1 part of the product of a bivector and vector.[13] In three dimensions bivectors are dual to vectors so the product is equivalent to the cross product, with the bivector instead of its vector dual. In higher dimensions the product can still be calculated but bivectors have more degrees of freedom and are not equivalent to vectors.[13]
This notation is also often much easier to work with, for example, in epipolar geometry.
From the general properties of the cross product follows immediately that





[

a


]

×




a

=

0



{\displaystyle [\mathbf {a} ]_{\times }\,\mathbf {a} =\mathbf {0} }

   and   





a



T




[

a


]

×


=

0



{\displaystyle \mathbf {a} ^{\mathrm {T} }\,[\mathbf {a} ]_{\times }=\mathbf {0} }



and from fact that [a]× is skew-symmetric it follows that







b



T




[

a


]

×




b

=
0.


{\displaystyle \mathbf {b} ^{\mathrm {T} }\,[\mathbf {a} ]_{\times }\,\mathbf {b} =0.}



The above-mentioned triple product expansion (bac–cab rule) can be easily proven using this notation.
As mentioned above, the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3), whose elements can be identified with the 3×3 skew-symmetric matrices. The map a → [a]× provides an isomorphism between R3 and so(3). Under this map, the cross product of 3-vectors corresponds to the commutator of 3x3 skew-symmetric matrices.




Matrix conversion for cross product with canonical base vectors


Denoting with 





e


i


∈


R


3
×
1




{\displaystyle \mathbf {e} _{i}\in \mathbf {R} ^{3\times 1}}

 the 



i


{\displaystyle i}

-th canonical base vector, the cross product of a generic vector 




v

∈


R


3
×
1




{\displaystyle \mathbf {v} \in \mathbf {R} ^{3\times 1}}

 with 





e


i




{\displaystyle \mathbf {e} _{i}}

 is given by: 




v

×


e


i


=


C


i



v



{\displaystyle \mathbf {v} \times \mathbf {e} _{i}=\mathbf {C} _{i}\mathbf {v} }

, where






C


1


=


[



0


0


0




0


0


1




0


−
1


0



]


,



C


2


=


[



0


0


−
1




0


0


0




1


0


0



]


,



C


3


=


[



0


1


0




−
1


0


0




0


0


0



]




{\displaystyle \mathbf {C} _{1}={\begin{bmatrix}0&0&0\\0&0&1\\0&-1&0\end{bmatrix}},\quad \mathbf {C} _{2}={\begin{bmatrix}0&0&-1\\0&0&0\\1&0&0\end{bmatrix}},\quad \mathbf {C} _{3}={\begin{bmatrix}0&1&0\\-1&0&0\\0&0&0\end{bmatrix}}}


These matrices share the following properties:







C


i


T


=
−


C


i




{\displaystyle \mathbf {C} _{i}^{T}=-\mathbf {C} _{i}}

 (skew-symmetric);
Both trace and determinant are zero;





rank

(


C


i


)
=
2


{\displaystyle {\text{rank}}(\mathbf {C} _{i})=2}

;






C


i




C


i


T


=


P




e


i







⊥






{\displaystyle \mathbf {C} _{i}\mathbf {C} _{i}^{T}=\mathbf {P} _{\mathbf {e} _{i}}^{^{\perp }}}

 (see below);

The orthogonal projection matrix of a vector 




v

≠

0



{\displaystyle \mathbf {v} \neq \mathbf {0} }

 is given by 





P



v



=

v

(


v


T



v


)

−
1




v


T




{\displaystyle \mathbf {P} _{\mathbf {v} }=\mathbf {v} (\mathbf {v} ^{T}\mathbf {v} )^{-1}\mathbf {v} ^{T}}

. The projection matrix onto the orthogonal complement is given by 





P



v






⊥




=

I

−


P



v





{\displaystyle \mathbf {P} _{\mathbf {v} }^{^{\perp }}=\mathbf {I} -\mathbf {P} _{\mathbf {v} }}

, where 




I



{\displaystyle \mathbf {I} }

 is the identity matrix. For the special case of 




v

=


e


i




{\displaystyle \mathbf {v} =\mathbf {e} _{i}}

, it can be verified that






P




e


1







⊥




=


[



0


0


0




0


1


0




0


0


1



]


,



P




e


2







⊥




=


[



1


0


0




0


0


0




0


0


1



]


,



P




e


3







⊥




=


[



1


0


0




0


1


0




0


0


0



]




{\displaystyle \mathbf {P} _{\mathbf {e} _{1}}^{^{\perp }}={\begin{bmatrix}0&0&0\\0&1&0\\0&0&1\end{bmatrix}},\quad \mathbf {P} _{\mathbf {e} _{2}}^{^{\perp }}={\begin{bmatrix}1&0&0\\0&0&0\\0&0&1\end{bmatrix}},\quad \mathbf {P} _{\mathbf {e} _{3}}^{^{\perp }}={\begin{bmatrix}1&0&0\\0&1&0\\0&0&0\end{bmatrix}}}


For other properties of orthogonal projection matrices, see projection (linear algebra).





Index notation for tensors[edit]
The cross product can alternatively be defined in terms of the Levi-Civita symbol εijk and a dot product ηmi (= δmi for an orthonormal basis), which are useful in converting vector notation for tensor applications:






a
×
b

=

c

⇔
 

c

m


=

∑

i
=
1


3



∑

j
=
1


3



∑

k
=
1


3



η

m
i



ε

i
j
k



a

j



b

k




{\displaystyle \mathbf {a\times b} =\mathbf {c} \Leftrightarrow \ c^{m}=\sum _{i=1}^{3}\sum _{j=1}^{3}\sum _{k=1}^{3}\eta ^{mi}\varepsilon _{ijk}a^{j}b^{k}}



where the indices 



i
,
j
,
k


{\displaystyle i,j,k}

 correspond to vector components. This characterization of the cross product is often expressed more compactly using the Einstein summation convention as






a
×
b

=

c

⇔
 

c

m


=

η

m
i



ε

i
j
k



a

j



b

k




{\displaystyle \mathbf {a\times b} =\mathbf {c} \Leftrightarrow \ c^{m}=\eta ^{mi}\varepsilon _{ijk}a^{j}b^{k}}



in which repeated indices are summed over the values 1 to 3. Note that this representation is another form of the skew-symmetric representation of the cross product:






η

m
i



ε

i
j
k



a

j


=
[

a


]

×


.


{\displaystyle \eta ^{mi}\varepsilon _{ijk}a^{j}=[\mathbf {a} ]_{\times }.}



In classical mechanics: representing the cross-product by using the Levi-Civita symbol can cause mechanical symmetries to be obvious when physical systems are isotropic. (An example: consider a particle in a Hooke's Law potential in three-space, free to oscillate in three dimensions; none of these dimensions are "special" in any sense, so symmetries lie in the cross-product-represented angular momentum, which are made clear by the abovementioned Levi-Civita representation).[citation needed]
Mnemonic[edit]
"Xyzzy (mnemonic)" redirects here. For other uses, see Xyzzy.
The word "xyzzy" can be used to remember the definition of the cross product.
If






a

=

b

×

c



{\displaystyle \mathbf {a} =\mathbf {b} \times \mathbf {c} }



where:






a

=


[




a

x







a

y







a

z





]


,

b

=


[




b

x







b

y







b

z





]


,

c

=


[




c

x







c

y







c

z





]




{\displaystyle \mathbf {a} ={\begin{bmatrix}a_{x}\\a_{y}\\a_{z}\end{bmatrix}},\mathbf {b} ={\begin{bmatrix}b_{x}\\b_{y}\\b_{z}\end{bmatrix}},\mathbf {c} ={\begin{bmatrix}c_{x}\\c_{y}\\c_{z}\end{bmatrix}}}



then:






a

x


=

b

y



c

z


−

b

z



c

y




{\displaystyle a_{x}=b_{y}c_{z}-b_{z}c_{y}}







a

y


=

b

z



c

x


−

b

x



c

z




{\displaystyle a_{y}=b_{z}c_{x}-b_{x}c_{z}}







a

z


=

b

x



c

y


−

b

y



c

x


.


{\displaystyle a_{z}=b_{x}c_{y}-b_{y}c_{x}.}



The second and third equations can be obtained from the first by simply vertically rotating the subscripts, x → y → z → x. The problem, of course, is how to remember the first equation, and two options are available for this purpose: either to remember the relevant two diagonals of Sarrus's scheme (those containing i), or to remember the xyzzy sequence.
Since the first diagonal in Sarrus's scheme is just the main diagonal of the above-mentioned 3×3 matrix, the first three letters of the word xyzzy can be very easily remembered.
Cross visualization[edit]
Similarly to the mnemonic device above, a "cross" or X can be visualized between the two vectors in the equation. This may be helpful for remembering the correct cross product formula.
If






a

=

b

×

c



{\displaystyle \mathbf {a} =\mathbf {b} \times \mathbf {c} }



then:






a

=


[




b

x







b

y







b

z





]


×


[




c

x







c

y







c

z





]


.


{\displaystyle \mathbf {a} ={\begin{bmatrix}b_{x}\\b_{y}\\b_{z}\end{bmatrix}}\times {\begin{bmatrix}c_{x}\\c_{y}\\c_{z}\end{bmatrix}}.}



If we want to obtain the formula for 




a

x




{\displaystyle a_{x}}

 we simply drop the 




b

x




{\displaystyle b_{x}}

 and 




c

x




{\displaystyle c_{x}}

 from the formula, and take the next two components down:






a

x


=


[




b

y







b

z





]


×


[




c

y







c

z





]


.


{\displaystyle a_{x}={\begin{bmatrix}b_{y}\\b_{z}\end{bmatrix}}\times {\begin{bmatrix}c_{y}\\c_{z}\end{bmatrix}}.}



It should be noted that when doing this for 




a

y




{\displaystyle a_{y}}

 the next two elements down should "wrap around" the matrix so that after the z component comes the x component. For clarity, when performing this operation for 




a

y




{\displaystyle a_{y}}

, the next two components should be z and x (in that order). While for 




a

z




{\displaystyle a_{z}}

 the next two components should be taken as x and y.






a

y


=


[




b

z







b

x





]


×


[




c

z







c

x





]


,

a

z


=


[




b

x







b

y





]


×


[




c

x







c

y





]




{\displaystyle a_{y}={\begin{bmatrix}b_{z}\\b_{x}\end{bmatrix}}\times {\begin{bmatrix}c_{z}\\c_{x}\end{bmatrix}},a_{z}={\begin{bmatrix}b_{x}\\b_{y}\end{bmatrix}}\times {\begin{bmatrix}c_{x}\\c_{y}\end{bmatrix}}}



For 




a

x




{\displaystyle a_{x}}

 then, if we visualize the cross operator as pointing from an element on the left to an element on the right, we can take the first element on the left and simply multiply by the element that the cross points to in the right hand matrix. We then subtract the next element down on the left, multiplied by the element that the cross points to here as well. This results in our 




a

x




{\displaystyle a_{x}}

 formula –






a

x


=

b

y



c

z


−

b

z



c

y


.


{\displaystyle a_{x}=b_{y}c_{z}-b_{z}c_{y}.}



We can do this in the same way for 




a

y




{\displaystyle a_{y}}

 and 




a

z




{\displaystyle a_{z}}

 to construct their associated formulas.
Applications[edit]
The cross product has applications in various contexts: e.g. it is used in computational geometry, physics and engineering. A non-exhaustive list of examples follows.
Computational geometry[edit]
The cross product appears in the calculation of the distance of two skew lines (lines not in the same plane) from each other in three-dimensional space.
The cross product can be used to calculate the normal for a triangle or polygon, an operation frequently performed in computer graphics. For example, the winding of a polygon (clockwise or anticlockwise) about a point within the polygon can be calculated by triangulating the polygon (like spoking a wheel) and summing the angles (between the spokes) using the cross product to keep track of the sign of each angle.
In computational geometry of the plane, the cross product is used to determine the sign of the acute angle defined by three points 




p

1


=
(

x

1


,

y

1


)
,

p

2


=
(

x

2


,

y

2


)


{\displaystyle p_{1}=(x_{1},y_{1}),p_{2}=(x_{2},y_{2})}

 and 




p

3


=
(

x

3


,

y

3


)


{\displaystyle p_{3}=(x_{3},y_{3})}

. It corresponds to the direction of the cross product of the two coplanar vectors defined by the pairs of points 




p

1


,

p

2




{\displaystyle p_{1},p_{2}}

 and 




p

1


,

p

3




{\displaystyle p_{1},p_{3}}

, i.e., by the sign of the expression





P
=
(

x

2


−

x

1


)
(

y

3


−

y

1


)
−
(

y

2


−

y

1


)
(

x

3


−

x

1


)
.


{\displaystyle P=(x_{2}-x_{1})(y_{3}-y_{1})-(y_{2}-y_{1})(x_{3}-x_{1}).}



In the "right-handed" coordinate system, if the result is 0, the points are collinear; if it is positive, the three points constitute a positive angle of rotation around 




p

1




{\displaystyle p_{1}}

 from 




p

2




{\displaystyle p_{2}}

 to 




p

3




{\displaystyle p_{3}}

, otherwise a negative angle. From another point of view, the sign of 



P


{\displaystyle P}

 tells whether 




p

3




{\displaystyle p_{3}}

 lies to the left or to the right of line 




p

1


,

p

2


.


{\displaystyle p_{1},p_{2}.}


The cross product is used in calculating the volume of a polyhedron such as a tetrahedron or parallelepiped.
Angular momentum and torque[edit]
The angular momentum 




L



{\displaystyle \mathbf {L} }

 of a particle about a given origin is defined as:






L

=

r

×

p

,


{\displaystyle \mathbf {L} =\mathbf {r} \times \mathbf {p} ,}



where 




r



{\displaystyle \mathbf {r} }

 is the position vector of the particle relative to the origin, 




p



{\displaystyle \mathbf {p} }

 is the linear momentum of the particle.
In the same way, the moment 




M



{\displaystyle \mathbf {M} }

 of a force 





F



B





{\displaystyle \mathbf {F} _{\mathrm {B} }}

 applied at point B around point A is given as:







M



A



=


r



A
B



×


F



B






{\displaystyle \mathbf {M} _{\mathrm {A} }=\mathbf {r} _{\mathrm {AB} }\times \mathbf {F} _{\mathrm {B} }\,}



In mechanics the moment of a force is also called torque and written as 




τ



{\displaystyle \mathbf {\tau } }


Since position 




r



{\displaystyle \mathbf {r} }

, linear momentum 




p



{\displaystyle \mathbf {p} }

 and force 




F



{\displaystyle \mathbf {F} }

 are all true vectors, both the angular momentum 




L



{\displaystyle \mathbf {L} }

 and the moment of a force 




M



{\displaystyle \mathbf {M} }

 are pseudovectors or axial vectors.
Rigid body[edit]
The cross product frequently appears in the description of rigid motions. Two points P and Q on a rigid body can be related by:







v


P


−


v


Q


=

ω

×

(


r


P


−


r


Q


)




{\displaystyle \mathbf {v} _{P}-\mathbf {v} _{Q}=\mathbf {\omega } \times \left(\mathbf {r} _{P}-\mathbf {r} _{Q}\right)\,}



where 




r



{\displaystyle \mathbf {r} }

 is the point's position, 




v



{\displaystyle \mathbf {v} }

 is its velocity and 




ω



{\displaystyle \mathbf {\omega } }

 is the body's angular velocity.
Since position 




r



{\displaystyle \mathbf {r} }

 and velocity 




v



{\displaystyle \mathbf {v} }

 are true vectors, the angular velocity 




ω



{\displaystyle \mathbf {\omega } }

 is a pseudovector or axial vector.
Lorentz force[edit]
See also: Lorentz force
The cross product is used to describe the Lorentz force experienced by a moving electric charge 




q

e




{\displaystyle q_{e}}

:






F

=

q

e



(

E

+

v

×

B

)



{\displaystyle \mathbf {F} =q_{e}\left(\mathbf {E} +\mathbf {v} \times \mathbf {B} \right)}



Since velocity 




v



{\displaystyle \mathbf {v} }

, force 




F



{\displaystyle \mathbf {F} }

 and electric field 




E



{\displaystyle \mathbf {E} }

 are all true vectors, the magnetic field 




B



{\displaystyle \mathbf {B} }

 is a pseudovector.
Other[edit]
In vector calculus, the cross product is used to define the formula for the vector operator curl.
The trick of rewriting a cross product in terms of a matrix multiplication appears frequently in epipolar and multi-view geometry, in particular when deriving matching constraints.
Cross product as an exterior product[edit]




The cross product in relation to the exterior product. In red are the orthogonal unit vector, and the "parallel" unit bivector.


The cross product can be viewed in terms of the exterior product. This view allows for a natural geometric interpretation of the cross product. In exterior algebra the exterior product (or wedge product) of two vectors is a bivector. A bivector is an oriented plane element, in much the same way that a vector is an oriented line element. Given two vectors a and b, one can view the bivector a ∧ b as the oriented parallelogram spanned by a and b. The cross product is then obtained by taking the Hodge dual of the bivector a ∧ b, mapping 2-vectors to vectors:





a
×
b
=
∗
(
a
∧
b
)

.


{\displaystyle a\times b=*(a\wedge b)\,.}



This can be thought of as the oriented multi-dimensional element "perpendicular" to the bivector. Only in three dimensions is the result an oriented line element – a vector – whereas, for example, in 4 dimensions the Hodge dual of a bivector is two-dimensional – another oriented plane element. So, only in three dimensions is the cross product of a and b the vector dual to the bivector a ∧ b: it is perpendicular to the bivector, with orientation dependent on the coordinate system's handedness, and has the same magnitude relative to the unit normal vector as a ∧ b has relative to the unit bivector; precisely the properties described above.
Cross product and handedness[edit]
When measurable quantities involve cross products, the handedness of the coordinate systems used cannot be arbitrary. However, when physics laws are written as equations, it should be possible to make an arbitrary choice of the coordinate system (including handedness). To avoid problems, one should be careful to never write down an equation where the two sides do not behave equally under all transformations that need to be considered. For example, if one side of the equation is a cross product of two vectors, one must take into account that when the handedness of the coordinate system is not fixed a priori, the result is not a (true) vector but a pseudovector. Therefore, for consistency, the other side must also be a pseudovector.[citation needed]
More generally, the result of a cross product may be either a vector or a pseudovector, depending on the type of its operands (vectors or pseudovectors). Namely, vectors and pseudovectors are interrelated in the following ways under application of the cross product:

vector × vector = pseudovector
pseudovector × pseudovector = pseudovector
vector × pseudovector = vector
pseudovector × vector = vector.

So by the above relationships, the unit basis vectors i, j and k of an orthonormal, right-handed (Cartesian) coordinate frame must all be pseudovectors (if a basis of mixed vector types is disallowed, as it normally is) since i × j = k, j × k = i and k × i = j.
Because the cross product may also be a (true) vector, it may not change direction with a mirror image transformation. This happens, according to the above relationships, if one of the operands is a (true) vector and the other one is a pseudovector (e.g., the cross product of two vectors). For instance, a vector triple product involving three (true) vectors is a (true) vector.
A handedness-free approach is possible using exterior algebra.
Generalizations[edit]
There are several ways to generalize the cross product to the higher dimensions.
Lie algebra[edit]
Main article: Lie algebra
The cross product can be seen as one of the simplest Lie products, and is thus generalized by Lie algebras, which are axiomatized as binary products satisfying the axioms of multilinearity, skew-symmetry, and the Jacobi identity. Many Lie algebras exist, and their study is a major field of mathematics, called Lie theory.
For example, the Heisenberg algebra gives another Lie algebra structure on 





R


3


.


{\displaystyle \mathbf {R} ^{3}.}

 In the basis 



{
x
,
y
,
z
}
,


{\displaystyle \{x,y,z\},}

 the product is 



[
x
,
y
]
=
z
,
[
x
,
z
]
=
[
y
,
z
]
=
0.


{\displaystyle [x,y]=z,[x,z]=[y,z]=0.}


Quaternions[edit]
Further information: quaternions and spatial rotation
The cross product can also be described in terms of quaternions, and this is why the letters i, j, k are a convention for the standard basis on R3. The unit vectors i, j, k correspond to "binary" (180 deg) rotations about their respective axes (Altmann, S. L., 1986, Ch. 12), said rotations being represented by "pure" quaternions (zero real part) with unit norms.
For instance, the above given cross product relations among i, j, and k agree with the multiplicative relations among the quaternions i, j, and k. In general, if a vector [a1, a2, a3] is represented as the quaternion a1i + a2j + a3k, the cross product of two vectors can be obtained by taking their product as quaternions and deleting the real part of the result. The real part will be the negative of the dot product of the two vectors.
Alternatively, using the above identification of the 'purely imaginary' quaternions with R3, the cross product may be thought of as half of the commutator of two quaternions.
Octonions[edit]
See also: Seven-dimensional cross product and Octonion
A cross product for 7-dimensional vectors can be obtained in the same way by using the octonions instead of the quaternions. The nonexistence of nontrivial vector-valued cross products of two vectors in other dimensions is related to the result from Hurwitz's theorem that the only normed division algebras are the ones with dimension 1, 2, 4, and 8.
Exterior product[edit]
Main article: Exterior algebra
In general dimension, there is no direct analogue of the binary cross product that yields specifically a vector. There is however the exterior product, which has similar properties, except that the exterior product of two vectors is now a 2-vector instead of an ordinary vector. As mentioned above, the cross product can be interpreted as the exterior product in three dimensions by using the Hodge dual to map 2-vectors to vectors. The Hodge dual of the exterior product yields an (n − 2)-vector, which is a natural generalization of the cross product in any number of dimensions.
The exterior product and dot product can be combined (through summation) to form the geometric product.
Multilinear algebra[edit]
In the context of multilinear algebra, the cross product can be seen as the (1,2)-tensor (a mixed tensor, specifically a bilinear map) obtained from the 3-dimensional volume form,[note 2] a (0,3)-tensor, by raising an index.
In detail, the 3-dimensional volume form defines a product 



V
×
V
×
V
→

R

,


{\displaystyle V\times V\times V\to \mathbf {R} ,}

 by taking the determinant of the matrix given by these 3 vectors. By duality, this is equivalent to a function 



V
×
V
→

V

∗


,


{\displaystyle V\times V\to V^{*},}

 (fixing any two inputs gives a function 



V
→

R



{\displaystyle V\to \mathbf {R} }

 by evaluating on the third input) and in the presence of an inner product (such as the dot product; more generally, a non-degenerate bilinear form), we have an isomorphism 



V
→

V

∗


,


{\displaystyle V\to V^{*},}

 and thus this yields a map 



V
×
V
→
V
,


{\displaystyle V\times V\to V,}

 which is the cross product: a (0,3)-tensor (3 vector inputs, scalar output) has been transformed into a (1,2)-tensor (2 vector inputs, 1 vector output) by "raising an index".
Translating the above algebra into geometry, the function "volume of the parallelepiped defined by 



(
a
,
b
,
−
)


{\displaystyle (a,b,-)}

" (where the first two vectors are fixed and the last is an input), which defines a function 



V
→

R



{\displaystyle V\to \mathbf {R} }

, can be represented uniquely as the dot product with a vector: this vector is the cross product 



a
×
b
.


{\displaystyle a\times b.}

 From this perspective, the cross product is defined by the scalar triple product, 




V
o
l

(
a
,
b
,
c
)
=
(
a
×
b
)
⋅
c
.


{\displaystyle \mathrm {Vol} (a,b,c)=(a\times b)\cdot c.}


In the same way, in higher dimensions one may define generalized cross products by raising indices of the n-dimensional volume form, which is a 



(
0
,
n
)


{\displaystyle (0,n)}

-tensor. The most direct generalizations of the cross product are to define either:

a 



(
1
,
n
−
1
)


{\displaystyle (1,n-1)}

-tensor, which takes as input 



n
−
1


{\displaystyle n-1}

 vectors, and gives as output 1 vector – an 



(
n
−
1
)


{\displaystyle (n-1)}

-ary vector-valued product, or
a 



(
n
−
2
,
2
)


{\displaystyle (n-2,2)}

-tensor, which takes as input 2 vectors and gives as output skew-symmetric tensor of rank n − 2 – a binary product with rank n − 2 tensor values. One can also define 



(
k
,
n
−
k
)


{\displaystyle (k,n-k)}

-tensors for other k.

These products are all multilinear and skew-symmetric, and can be defined in terms of the determinant and parity.
The 



(
n
−
1
)


{\displaystyle (n-1)}

-ary product can be described as follows: given 



n
−
1


{\displaystyle n-1}

 vectors 




v

1


,
…
,

v

n
−
1




{\displaystyle v_{1},\dots ,v_{n-1}}

 in 





R


n


,


{\displaystyle \mathbf {R} ^{n},}

 define their generalized cross product 




v

n


=

v

1


×
⋯
×

v

n
−
1




{\displaystyle v_{n}=v_{1}\times \cdots \times v_{n-1}}

 as:

perpendicular to the hyperplane defined by the 




v

i


,


{\displaystyle v_{i},}


magnitude is the volume of the parallelotope defined by the 




v

i


,


{\displaystyle v_{i},}

 which can be computed as the Gram determinant of the 




v

i


,


{\displaystyle v_{i},}


oriented so that 




v

1


,
…
,

v

n




{\displaystyle v_{1},\dots ,v_{n}}

 is positively oriented.

This is the unique multilinear, alternating product which evaluates to 




e

1


×
⋯
×

e

n
−
1


=

e

n




{\displaystyle e_{1}\times \cdots \times e_{n-1}=e_{n}}

, 




e

2


×
⋯
×

e

n


=

e

1


,


{\displaystyle e_{2}\times \cdots \times e_{n}=e_{1},}

 and so forth for cyclic permutations of indices.
In coordinates, one can give a formula for this 



(
n
−
1
)


{\displaystyle (n-1)}

-ary analogue of the cross product in Rn by:





⋀
(


v


1


,
…
,


v


n
−
1


)
=


|




v

1






1




⋯



v

1






n






⋮


⋱


⋮





v

n
−
1






1




⋯



v

n
−
1






n








e


1




⋯




e


n





|


.


{\displaystyle \bigwedge (\mathbf {v} _{1},\dots ,\mathbf {v} _{n-1})={\begin{vmatrix}v_{1}{}^{1}&\cdots &v_{1}{}^{n}\\\vdots &\ddots &\vdots \\v_{n-1}{}^{1}&\cdots &v_{n-1}{}^{n}\\\mathbf {e} _{1}&\cdots &\mathbf {e} _{n}\end{vmatrix}}.}



This formula is identical in structure to the determinant formula for the normal cross product in R3 except that the row of basis vectors is the last row in the determinant rather than the first. The reason for this is to ensure that the ordered vectors (v1, ...,vn−1, Λ(v1, ...,vn−1)) have a positive orientation with respect to (e1, ..., en). If n is odd, this modification leaves the value unchanged, so this convention agrees with the normal definition of the binary product. In the case that n is even, however, the distinction must be kept. This 



(
n
−
1
)


{\displaystyle (n-1)}

-ary form enjoys many of the same properties as the vector cross product: it is alternating and linear in its arguments, it is perpendicular to each argument, and its magnitude gives the hypervolume of the region bounded by the arguments. And just like the vector cross product, it can be defined in a coordinate independent way as the Hodge dual of the wedge product of the arguments.
Skew-symmetric matrix[edit]
If the cross product is defined as a binary operation, it takes as input exactly two vectors. If its output is not required to be a vector or a pseudovector but instead a matrix, then it can be generalized in an arbitrary number of dimensions.[14][15][16]
In mechanics, for example, the angular velocity can be interpreted either as a pseudovector 



ω


{\displaystyle \omega }

 or as a anti-symmetric matrix or skew-symmetric tensor 



Ω


{\displaystyle \Omega }

. In the latter case, the velocity law for a rigid body looks:







v


P


−


v


Q


=

Ω

⋅

(


r


P


−


r


Q


)



{\displaystyle \mathbf {v} _{P}-\mathbf {v} _{Q}={\Omega }\cdot \left(\mathbf {r} _{P}-\mathbf {r} _{Q}\right)}



where Ω is formally defined from the rotation matrix 




R

N
×
N




{\displaystyle R^{N\times N}}

 associated to body's frame: 



Ω
≜



d
R


d
t




R


T



.


{\displaystyle \Omega \triangleq {\frac {dR}{dt}}R^{\mathrm {T} }.}

 In three-dimensions holds:





Ω
=
[
ω

]

×


=


[





0



−

ω

3








ω

2










ω

3




0



−

ω

1







−

ω

2







ω

1






0



]




{\displaystyle \Omega =[\omega ]_{\times }={\begin{bmatrix}\,\,0&\!-\omega _{3}&\,\,\,\omega _{2}\\\,\,\,\omega _{3}&0&\!-\omega _{1}\\\!-\omega _{2}&\,\,\omega _{1}&\,\,0\end{bmatrix}}}



In quantum mechanics the angular momentum 



L


{\displaystyle L}

 is often represented as an anti-symmetric matrix or tensor. More precisely, it is the result of cross product involving position 




x



{\displaystyle \mathbf {x} }

 and linear momentum 




p



{\displaystyle \mathbf {p} }

:






L

i
j


=

x

i



p

j


−

p

i



x

j




{\displaystyle L_{ij}=x_{i}p_{j}-p_{i}x_{j}}



Since both 




x



{\displaystyle \mathbf {x} }

 and 




p



{\displaystyle \mathbf {p} }

 can have an arbitrary number 



N


{\displaystyle N}

 of components, that kind of cross product can be extended to any dimension, holding the "physical" interpretation of the operation.
See § Alternative ways to compute the cross product for numerical details.
History[edit]
In 1773, the Italian mathematician Joseph Louis Lagrange, (born Giuseppe Luigi Lagrancia), introduced the component form of both the dot and cross products in order to study the tetrahedron in three dimensions.[17] In 1843 the Irish mathematical physicist Sir William Rowan Hamilton introduced the quaternion product, and with it the terms "vector" and "scalar". Given two quaternions [0, u] and [0, v], where u and v are vectors in R3, their quaternion product can be summarized as [−u ⋅ v, u × v]. James Clerk Maxwell used Hamilton's quaternion tools to develop his famous electromagnetism equations, and for this and other reasons quaternions for a time were an essential part of physics education.
In 1878 William Kingdon Clifford published his Elements of Dynamic which was an advanced text for its time. He defined the product of two vectors[18] to have magnitude equal to the area of the parallelogram of which they are two sides, and direction perpendicular to their plane.
Oliver Heaviside in England and Josiah Willard Gibbs, a professor at Yale University in Connecticut, also felt that quaternion methods were too cumbersome, often requiring the scalar or vector part of a result to be extracted. Thus, about forty years after the quaternion product, the dot product and cross product were introduced—to heated opposition. Pivotal to (eventual) acceptance was the efficiency of the new approach, allowing Heaviside to reduce the equations of electromagnetism from Maxwell's original 20 to the four commonly seen today.[19]
Largely independent of this development, and largely unappreciated at the time, Hermann Grassmann created a geometric algebra not tied to dimension two or three, with the exterior product playing a central role. In 1853 Augustin-Louis Cauchy, a contemporary of Grassmann, published a paper on algebraic keys which were used to solve equations and had the same multiplication properties as the cross product.[20][21] William Kingdon Clifford combined the algebras of Hamilton and Grassmann to produce Clifford algebra, where in the case of three-dimensional vectors the bivector produced from two vectors dualizes to a vector, thus reproducing the cross product.
The cross notation and the name "cross product" began with Gibbs. Originally they appeared in privately published notes for his students in 1881 as Elements of Vector Analysis. The utility for mechanics was noted by Aleksandr Kotelnikov. Gibbs's notation and the name "cross product" later reached a wide audience through Vector Analysis, a textbook by Edwin Bidwell Wilson, a former student. Wilson rearranged material from Gibbs's lectures, together with material from publications by Heaviside, Föpps, and Hamilton. He divided vector analysis into three parts:

First, that which concerns addition and the scalar and vector products of vectors. Second, that which concerns the differential and integral calculus in its relations to scalar and vector functions. Third, that which contains the theory of the linear vector function.

Two main kinds of vector multiplications were defined, and they were called as follows:

The direct, scalar, or dot product of two vectors
The skew, vector, or cross product of two vectors

Several kinds of triple products and products of more than three vectors were also examined. The above-mentioned triple product expansion was also included.
See also[edit]


Geometry portal



Bivector
Cartesian product – A product of two sets
Dot Product
Exterior algebra
Multiple cross products – Products involving more than three vectors
Pseudovector
× (the symbol)

Notes[edit]



^ Here, "formal" means that this notation has the form of a determinant, but does not strictly adhere to the definition; it is a mnemonic used to remember the expansion of the cross product.
^ By a volume form one means a function that takes in n vectors and gives out a scalar, the volume of the parallelotope defined by the vectors: 



V
×
⋯
×
V
→

R

.


{\displaystyle V\times \cdots \times V\to \mathbf {R} .}

 This is an n-ary multilinear skew-symmetric form. In the presence of a basis, such as on 





R


n


,


{\displaystyle \mathbf {R} ^{n},}

 this is given by the determinant, but in an abstract vector space, this is added structure. In terms of G-structures, a volume form is an 



S
L


{\displaystyle SL}

-structure.



References[edit]


^ WS Massey (1983). "Cross products of vectors in higher dimensional Euclidean spaces". The American Mathematical Monthly. 90 (10): 697–701. JSTOR 2323537. doi:10.2307/2323537. If one requires only three basic properties of the cross product ... it turns out that a cross product of vectors exists only in 3-dimensional and 7-dimensional Euclidean space. 
^ Jeffreys, H; Jeffreys, BS (1999). Methods of mathematical physics. Cambridge University Press. 
^ Wilson 1901, p. 60–61
^ Dennis G. Zill; Michael R. Cullen (2006). "Definition 7.4: Cross product of two vectors". Advanced engineering mathematics (3rd ed.). Jones & Bartlett Learning. p. 324. ISBN 0-7637-4591-X. 
^ a b A History of Vector Analysis by Michael J. Crowe, Math. UC Davis
^ Dennis G. Zill; Michael R. Cullen (2006). "Equation 7: a × b as sum of determinants". cited work. Jones & Bartlett Learning. p. 321. ISBN 0-7637-4591-X. 
^ M. R. Spiegel; S. Lipschutz; D. Spellman (2009). Vector Analysis. Schaum's outlines. McGraw Hill. p. 29. ISBN 978-0-07-161545-7. 
^ WS Massey (Dec 1983). "Cross products of vectors in higher dimensional Euclidean spaces". The American Mathematical Monthly. The American Mathematical Monthly, Vol. 90, No. 10. 90 (10): 697–701. JSTOR 2323537. doi:10.2307/2323537. 
^ Vladimir A. Boichenko; Gennadiĭ Alekseevich Leonov; Volker Reitmann (2005). Dimension theory for ordinary differential equations. Vieweg+Teubner Verlag. p. 26. ISBN 3-519-00437-2. 
^ Pertti Lounesto (2001). Clifford algebras and spinors (2nd ed.). Cambridge University Press. p. 94. ISBN 0-521-00551-5. 
^ a b Shuangzhe Liu; Gõtz Trenkler (2008). "Hadamard, Khatri-Rao, Kronecker and other matrix products" (PDF). Int J Information and systems sciences. Institute for scientific computing and education. 4 (1): 160–177. 
^ by Eric W. Weisstein (2003). "Binet-Cauchy identity". CRC concise encyclopedia of mathematics (2nd ed.). CRC Press. p. 228. ISBN 1-58488-347-2. 
^ a b Lounesto, Pertti (2001). Clifford algebras and spinors. Cambridge: Cambridge University Press. p. 193. ISBN 978-0-521-00551-7. 
^ A. W. McDavid; C. D. McMullen (2006). "Generalizing Cross Products and Maxwell's Equations to Universal Extra Dimensions" (PDF). 
^ C. A. Gonano (2011). Estensione in N-D di prodotto vettore e rotore e loro applicazioni (PDF). Politecnico di Milano, Italy. 
^ C. A. Gonano; R. E. Zich (2014). "Cross product in N Dimensions – the doublewedge product" (PDF). 
^ Lagrange, JL (1773). "Solutions analytiques de quelques problèmes sur les pyramides triangulaires". Oeuvres. vol 3. 
^ William Kingdon Clifford (1878) Elements of Dynamic[permanent dead link], Part I, page 95, London: MacMillan & Co; online presentation by Cornell University Historical Mathematical Monographs
^ Nahin, Paul J. (2000). Oliver Heaviside: the life, work, and times of an electrical genius of the Victorian age. JHU Press. pp. 108–109. ISBN 0-8018-6909-9. 
^ Crowe, Michael J. (1994). A History of Vector Analysis. Dover. p. 83. ISBN 0-486-67910-1. 
^ Cauchy, Augustin-Louis (1900). Ouvres. 12. p. 16. 



Cajori, Florian (1929). A History Of Mathematical Notations Volume II. Open Court Publishing. p.  134. ISBN 978-0-486-67766-8. 
E. A. Milne (1948) Vectorial Mechanics, Chapter 2: Vector Product, pp 11 –31, London: Methuen Publishing.
Wilson, Edwin Bidwell (1901). Vector Analysis: A text-book for the use of students of mathematics and physics, founded upon the lectures of J. Willard Gibbs. Yale University Press. 
T. Levi-Civita; U. Amaldi (1949). Lezioni di meccanica razionale (in Italian). Bologna: Zanichelli editore. 

External links[edit]

Hazewinkel, Michiel, ed. (2001), "Cross product", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W. "Cross Product". MathWorld. 


A quick geometrical derivation and interpretation of cross products
Gonano, Carlo Andrea; Zich, Riccardo Enrico (21 July 2014). "Cross product in N Dimensions – the doublewedge product". arXiv:1408.5799  [math.GM].  Polytechnic University of Milan, Italy.
Silagadze, Zurab K. (30 April 2002). "Multi-dimensional vector product". arXiv:math/0204357  [math.RA]. . Journal of Physics. A35, 4949 (it is only possible in 7-D space)
Real and Complex Products of Complex Numbers
An interactive tutorial created at Syracuse University – (requires java)
W. Kahan (2007). Cross-Products and Rotations in Euclidean 2- and 3-Space. University of California, Berkeley (PDF).







v
t
e


Linear algebra



Basic concepts



Scalar
Vector
Vector space
Scalar multiplication
Vector projection
Linear span
Linear map
Linear projection
Linear independence
Linear combination
Basis
Column space
Row space
Dual space
Orthogonality
Kernel
Eigenvalues and eigenvectors
Outer product
Inner product space
Dot product
Transpose
Gram–Schmidt process
Linear equations





Vector algebra



Cross product
Triple product
Seven-dimensional cross product





Multilinear algebra



Geometric algebra
Exterior algebra
Bivector
Multivector





Matrices



Block
Decomposition
Invertible
Minor
Multiplication
Rank
Transformation
Cramer's rule
Gaussian elimination





Numerical



Floating point
Matrix Laboratory
Numerical stability
Basic Linear Algebra Subprograms (BLAS)
Sparse matrix
Comparison of linear algebra libraries
Comparison of numerical analysis software










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Cross_product&oldid=793466453"					
Categories: Bilinear operatorsBinary operationsLinear algebraAnalytic geometryVectors (mathematics and physics)Hidden categories: All articles with dead external linksArticles with dead external links from July 2016Articles with permanently dead external linksAll articles with unsourced statementsArticles with unsourced statements from November 2009Articles with unsourced statements from April 2008CS1 Italian-language sources (it) 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


AlemannischአማርኛالعربيةБългарскиBosanskiCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolEsperantoEuskaraفارسیFrançaisGalego한국어हिन्दीBahasa IndonesiaÍslenskaItalianoעבריתქართულიҚазақшаLatviešuLietuviųMagyarമലയാളംमराठीNederlands日本語Norsk bokmålNorsk nynorskOʻzbekcha/ўзбекчаਪੰਜਾਬੀPiemontèisPolskiPortuguêsRomânăРусскийScotsSimple EnglishSlovenčinaSlovenščinaSuomiSvenskaTagalogதமிழ்ไทยTürkçeУкраїнськаTiếng Việt中文 
Edit links 





 This page was last edited on 1 August 2017, at 22:38.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 








Vector Products Inc. Home










Vector Products Inc.


vectorproducts.ca



905-564-3535will@vectorproducts.ca









Name:








Email:








Question/Comment:





















1585 Britania Rd. E Unit E-9MississaugaOntarioL4W2M4


View map.


Our Address


Contact Us


Ask us a question


Find Us











Dot product - Wikipedia





















 






Dot product

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search

"Scalar product" redirects here. For the abstract scalar product, see Inner product space. For the product of a vector and a scalar, see Scalar multiplication.
In mathematics, the dot product or scalar product[note 1] is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. In Euclidean geometry, the dot product of the Cartesian coordinates of two vectors is widely used and often called inner product (or rarely projection product); see also inner product space.
Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. These definitions are equivalent when using Cartesian coordinates. In modern geometry, Euclidean spaces are often defined by using vector spaces. In this case, the dot product is used for defining lengths (the length of a vector is the square root of the dot product of the vector by itself) and angles (the cosine of the angle of two vectors is the quotient of their dot product by the product of their lengths).
The name "dot product" is derived from the centered dot " · " that is often used to designate this operation; the alternative name "scalar product" emphasizes that the result is a scalar, rather than a vector, which is the case for the vector product in three-dimensional space.



Contents


1 Definition

1.1 Algebraic definition
1.2 Geometric definition
1.3 Scalar projection and first properties
1.4 Equivalence of the definitions


2 Properties

2.1 Application to the law of cosines


3 Triple product expansion
4 Physics
5 Generalizations

5.1 Complex vectors
5.2 Inner product
5.3 Functions
5.4 Weight function
5.5 Dyadics and matrices
5.6 Tensors


6 Computation

6.1 Algorithms
6.2 Libraries


7 See also
8 Notes
9 References
10 External links



Definition[edit]
The dot product may be defined algebraically or geometrically. The geometric definition is based on the notions of angle and distance (magnitude of vectors). The equivalence of these two definitions relies on having a Cartesian coordinate system for Euclidean space.
In modern presentations of Euclidean geometry, the points of space are defined in terms of their Cartesian coordinates, and Euclidean space itself is commonly identified with the real coordinate space Rn. In such a presentation, the notions of length and angles are not primitive.[clarification needed] They are defined by means of the dot product: the length of a vector is defined as the square root of the dot product of the vector by itself, and the cosine of the (non oriented) angle of two vectors of length one is defined as their dot product. So the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of Euclidean geometry.[citation needed]
Algebraic definition[edit]
The dot product of two vectors a = [a1, a2, ..., an] and b = [b1, b2, ..., bn] is defined as:[1]






a

⋅

b

=

∑

i
=
1


n



a

i



b

i


=

a

1



b

1


+

a

2



b

2


+
⋯
+

a

n



b

n




{\displaystyle \mathbf {a} \cdot \mathbf {b} =\sum _{i=1}^{n}a_{i}b_{i}=a_{1}b_{1}+a_{2}b_{2}+\cdots +a_{n}b_{n}}



where Σ denotes summation notation and n is the dimension of the vector space. For instance, in three-dimensional space, the dot product of vectors [1, 3, −5] and [4, −2, −1] is:









 
[
1
,
3
,
−
5
]
⋅
[
4
,
−
2
,
−
1
]



=
(
1
)
(
4
)
+
(
3
)
(
−
2
)
+
(
−
5
)
(
−
1
)






=
4
−
6
+
5






=
3






{\displaystyle {\begin{aligned}\ [1,3,-5]\cdot [4,-2,-1]&=(1)(4)+(3)(-2)+(-5)(-1)\\&=4-6+5\\&=3\end{aligned}}}



The dot product can also be written as:





a

⋅

b

=

|


a



b


T



|



{\displaystyle \mathbf {a} \cdot \mathbf {b} =|\mathbf {a} \mathbf {b} ^{T}|}

.
Here, 





b


T




{\displaystyle \mathbf {b} ^{T}}

means the transpose of 




b



{\displaystyle \mathbf {b} }

.
Using the above example, a 1 × 3 matrix (row vector) is multiplied by a 3 × 1 matrix (column vector) to get the result (1 × 1 matrix is obtained by matrix multiplication, which is a scalar):






|



[



1


3


−
5



]





[



4


−
2


−
1



]



T



|

=

|

3

|

=
3


{\displaystyle |{\begin{bmatrix}1&3&-5\end{bmatrix}}{\begin{bmatrix}4&-2&-1\end{bmatrix}}^{T}|=|3|=3}

.

Geometric definition[edit]
In Euclidean space, a Euclidean vector is a geometrical object that possesses both a magnitude and a direction. A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction that the arrow points. The magnitude of a vector a is denoted by 




∥

a

∥



{\displaystyle \left\|\mathbf {a} \right\|}

. The dot product of two Euclidean vectors a and b is defined by[2][3]






a

⋅

b

=
∥

a

∥
 
∥

b

∥
cos
⁡
(
θ
)
,


{\displaystyle \mathbf {a} \cdot \mathbf {b} =\|\mathbf {a} \|\ \|\mathbf {b} \|\cos(\theta ),}



where θ is the angle between a and b.
In particular, if a and b are orthogonal, then the angle between them is 90° and






a

⋅

b

=
0.


{\displaystyle \mathbf {a} \cdot \mathbf {b} =0.}



At the other extreme, if they are codirectional, then the angle between them is 0° and






a

⋅

b

=

∥

a

∥



∥

b

∥



{\displaystyle \mathbf {a} \cdot \mathbf {b} =\left\|\mathbf {a} \right\|\,\left\|\mathbf {b} \right\|}



This implies that the dot product of a vector a with itself is






a

⋅

a

=


∥

a

∥


2


,


{\displaystyle \mathbf {a} \cdot \mathbf {a} =\left\|\mathbf {a} \right\|^{2},}



which gives






∥

a

∥

=



a

⋅

a



,


{\displaystyle \left\|\mathbf {a} \right\|={\sqrt {\mathbf {a} \cdot \mathbf {a} }},}



the formula for the Euclidean length of the vector.
Scalar projection and first properties[edit]




Scalar projection


The scalar projection (or scalar component) of a Euclidean vector a in the direction of a Euclidean vector b is given by






a

b


=

∥

a

∥

cos
⁡
θ
,


{\displaystyle a_{b}=\left\|\mathbf {a} \right\|\cos \theta ,}



where θ is the angle between a and b.
In terms of the geometric definition of the dot product, this can be rewritten






a

b


=

a

⋅




b

^



,


{\displaystyle a_{b}=\mathbf {a} \cdot {\widehat {\mathbf {b} }},}



where 







b

^



=

b


/


∥

b

∥



{\displaystyle {\widehat {\mathbf {b} }}=\mathbf {b} /\left\|\mathbf {b} \right\|}

 is the unit vector in the direction of b.




Distributive law for the dot product


The dot product is thus characterized geometrically by[4]






a

⋅

b

=

a

b



∥

b

∥

=

b

a



∥

a

∥

.


{\displaystyle \mathbf {a} \cdot \mathbf {b} =a_{b}\left\|\mathbf {b} \right\|=b_{a}\left\|\mathbf {a} \right\|.}



The dot product, defined in this manner, is homogeneous under scaling in each variable, meaning that for any scalar α,





(
α

a

)
⋅

b

=
α
(

a

⋅

b

)
=

a

⋅
(
α

b

)
.


{\displaystyle (\alpha \mathbf {a} )\cdot \mathbf {b} =\alpha (\mathbf {a} \cdot \mathbf {b} )=\mathbf {a} \cdot (\alpha \mathbf {b} ).}



It also satisfies a distributive law, meaning that






a

⋅
(

b

+

c

)
=

a

⋅

b

+

a

⋅

c

.


{\displaystyle \mathbf {a} \cdot (\mathbf {b} +\mathbf {c} )=\mathbf {a} \cdot \mathbf {b} +\mathbf {a} \cdot \mathbf {c} .}



These properties may be summarized by saying that the dot product is a bilinear form. Moreover, this bilinear form is positive definite, which means that 




a

⋅

a



{\displaystyle \mathbf {a} \cdot \mathbf {a} }

 is never negative and is zero if and only if 




a

=

0

.


{\displaystyle \mathbf {a} =\mathbf {0} .}


Equivalence of the definitions[edit]
If e1, ..., en are the standard basis vectors in Rn, then we may write










a




=
[

a

1


,
…
,

a

n


]
=

∑

i



a

i




e


i







b




=
[

b

1


,
…
,

b

n


]
=

∑

i



b

i




e


i


.






{\displaystyle {\begin{aligned}\mathbf {a} &=[a_{1},\dots ,a_{n}]=\sum _{i}a_{i}\mathbf {e} _{i}\\\mathbf {b} &=[b_{1},\dots ,b_{n}]=\sum _{i}b_{i}\mathbf {e} _{i}.\end{aligned}}}



The vectors ei are an orthonormal basis, which means that they have unit length and are at right angles to each other. Hence since these vectors have unit length







e


i


⋅


e


i


=
1


{\displaystyle \mathbf {e} _{i}\cdot \mathbf {e} _{i}=1}



and since they form right angles with each other, if i ≠ j,







e


i


⋅


e


j


=
0.


{\displaystyle \mathbf {e} _{i}\cdot \mathbf {e} _{j}=0.}



Thus in general we can say that:







e


i


⋅


e


j


=

δ

i
j


.


{\displaystyle \mathbf {e} _{i}\cdot \mathbf {e} _{j}=\delta _{ij}.}



Where δ ij is the Kronecker delta.
Also, by the geometric definition, for any vector ei and a vector a, we note






a

⋅


e


i


=

∥

a

∥



∥


e


i


∥

cos
⁡
θ
=

∥

a

∥

cos
⁡
θ
=

a

i


,


{\displaystyle \mathbf {a} \cdot \mathbf {e} _{i}=\left\|\mathbf {a} \right\|\,\left\|\mathbf {e} _{i}\right\|\cos \theta =\left\|\mathbf {a} \right\|\cos \theta =a_{i},}



where ai is the component of vector a in the direction of ei.
Now applying the distributivity of the geometric version of the dot product gives






a

⋅

b

=

a

⋅

∑

i



b

i




e


i


=

∑

i



b

i


(

a

⋅


e


i


)
=

∑

i



b

i



a

i


=

∑

i



a

i



b

i


,


{\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {a} \cdot \sum _{i}b_{i}\mathbf {e} _{i}=\sum _{i}b_{i}(\mathbf {a} \cdot \mathbf {e} _{i})=\sum _{i}b_{i}a_{i}=\sum _{i}a_{i}b_{i},}



which is precisely the algebraic definition of the dot product. So the (geometric) dot product equals the (algebraic) dot product.
Properties[edit]
The dot product fulfills the following properties if a, b, and c are real vectors and r is a scalar.[1][2]

Commutative:






a

⋅

b

=

b

⋅

a

,


{\displaystyle \mathbf {a} \cdot \mathbf {b} =\mathbf {b} \cdot \mathbf {a} ,}


which follows from the definition (θ is the angle between a and b):





a

⋅

b

=

∥

a

∥


∥

b

∥

cos
⁡
θ
=

∥

b

∥


∥

a

∥

cos
⁡
θ
=

b

⋅

a

.


{\displaystyle \mathbf {a} \cdot \mathbf {b} =\left\|\mathbf {a} \right\|\left\|\mathbf {b} \right\|\cos \theta =\left\|\mathbf {b} \right\|\left\|\mathbf {a} \right\|\cos \theta =\mathbf {b} \cdot \mathbf {a} .}




Distributive over vector addition:






a

⋅
(

b

+

c

)
=

a

⋅

b

+

a

⋅

c

.


{\displaystyle \mathbf {a} \cdot (\mathbf {b} +\mathbf {c} )=\mathbf {a} \cdot \mathbf {b} +\mathbf {a} \cdot \mathbf {c} .}




Bilinear:






a

⋅
(
r

b

+

c

)
=
r
(

a

⋅

b

)
+
(

a

⋅

c

)
.


{\displaystyle \mathbf {a} \cdot (r\mathbf {b} +\mathbf {c} )=r(\mathbf {a} \cdot \mathbf {b} )+(\mathbf {a} \cdot \mathbf {c} ).}




Scalar multiplication:





(

c

1



a

)
⋅
(

c

2



b

)
=

c

1



c

2


(

a

⋅

b

)
.


{\displaystyle (c_{1}\mathbf {a} )\cdot (c_{2}\mathbf {b} )=c_{1}c_{2}(\mathbf {a} \cdot \mathbf {b} ).}




Not associative because the dot product between a scalar (a ⋅ b) and a vector (c) is not defined, which means that the expressions involved in the associative property, (a ⋅ b) ⋅ c or a ⋅ (b ⋅ c) are both ill-defined.[5] Note however that the previously mentioned scalar multiplication property is sometimes called the "associative law for scalar and dot product"[6] or one can say that "the dot product is associative with respect to scalar multiplication" because c (a ⋅ b) = (c a) ⋅ b = a ⋅ (c b).[7]
Orthogonal:

Two non-zero vectors a and b are orthogonal if and only if a ⋅ b = 0.


No cancellation:

Unlike multiplication of ordinary numbers, where if ab = ac, then b always equals c unless a is zero, the dot product does not obey the cancellation law:
If a ⋅ b = a ⋅ c and a ≠ 0, then we can write: a ⋅ (b − c) = 0 by the distributive law; the result above says this just means that a is perpendicular to (b − c), which still allows (b − c) ≠ 0, and therefore b ≠ c.


Product Rule: If a and b are functions, then the derivative (denoted by a prime ′) of a ⋅ b is a′ ⋅ b + a ⋅ b′.

Application to the law of cosines[edit]




Triangle with vector edges a and b, separated by angle θ.


Main article: Law of cosines
Given two vectors a and b separated by angle θ (see image right), they form a triangle with a third side c = a − b. The dot product of this with itself is:










c

⋅

c




=
(

a

−

b

)
⋅
(

a

−

b

)






=

a

⋅

a

−

a

⋅

b

−

b

⋅

a

+

b

⋅

b







=

a

2


−

a

⋅

b

−

a

⋅

b

+

b

2








=

a

2


−
2

a

⋅

b

+

b

2







c

2





=

a

2


+

b

2


−
2
a
b
cos
⁡
θ






{\displaystyle {\begin{aligned}\mathbf {c} \cdot \mathbf {c} &=(\mathbf {a} -\mathbf {b} )\cdot (\mathbf {a} -\mathbf {b} )\\&=\mathbf {a} \cdot \mathbf {a} -\mathbf {a} \cdot \mathbf {b} -\mathbf {b} \cdot \mathbf {a} +\mathbf {b} \cdot \mathbf {b} \\&=a^{2}-\mathbf {a} \cdot \mathbf {b} -\mathbf {a} \cdot \mathbf {b} +b^{2}\\&=a^{2}-2\mathbf {a} \cdot \mathbf {b} +b^{2}\\c^{2}&=a^{2}+b^{2}-2ab\cos \theta \\\end{aligned}}}



which is the law of cosines.

Triple product expansion[edit]
Main article: Triple product
This is an identity (also known as Lagrange's formula) involving the dot- and cross-products. It is written as:[1][2]






a

×
(

b

×

c

)
=

b

(

a

⋅

c

)
−

c

(

a

⋅

b

)
,


{\displaystyle \mathbf {a} \times (\mathbf {b} \times \mathbf {c} )=\mathbf {b} (\mathbf {a} \cdot \mathbf {c} )-\mathbf {c} (\mathbf {a} \cdot \mathbf {b} ),}



which may be remembered as "BAC minus CAB", keeping in mind which vectors are dotted together. This formula finds application in simplifying vector calculations in physics.
Physics[edit]
In physics,vector magnitude is a scalar in the physical sense, i.e. a physical quantity independent of the coordinate system, expressed as the product of a numerical value and a physical unit, not just a number. The dot product is also a scalar in this sense, given by the formula, independent of the coordinate system. Examples include:[8][9]

Mechanical work is the dot product of force and displacement vectors.
Magnetic flux is the dot product of the magnetic field and the vector area.

Generalizations[edit]
Complex vectors[edit]
For vectors with complex entries, using the given definition of the dot product would lead to quite different properties. For instance the dot product of a vector with itself would be an arbitrary complex number, and could be zero without the vector being the zero vector (such vectors are called isotropic); this in turn would have consequences for notions like length and angle. Properties such as the positive-definite norm can be salvaged at the cost of giving up the symmetric and bilinear properties of the scalar product, through the alternative definition[1]






a

⋅

b

=
∑


a

i





b

i


¯



,


{\displaystyle \mathbf {a} \cdot \mathbf {b} =\sum {a_{i}{\overline {b_{i}}}},}



where bi is the complex conjugate of bi. Then the scalar product of any vector with itself is a non-negative real number, and it is nonzero except for the zero vector. However this scalar product is thus sesquilinear rather than bilinear: it is conjugate linear and not linear in b, and the scalar product is not symmetric, since






a

⋅

b

=




b

⋅

a


¯


.


{\displaystyle \mathbf {a} \cdot \mathbf {b} ={\overline {\mathbf {b} \cdot \mathbf {a} }}.}



The angle between two complex vectors is then given by





cos
⁡
θ
=



Re
⁡
(

a

⋅

b

)



∥

a

∥



∥

b

∥




.


{\displaystyle \cos \theta ={\frac {\operatorname {Re} (\mathbf {a} \cdot \mathbf {b} )}{\left\|\mathbf {a} \right\|\,\left\|\mathbf {b} \right\|}}.}



This type of scalar product is nevertheless useful, and leads to the notions of Hermitian form and of general inner product spaces.
Inner product[edit]
Main article: Inner product space
The inner product generalizes the dot product to abstract vector spaces over a field of scalars, being either the field of real numbers 




R



{\displaystyle \mathbb {R} }

 or the field of complex numbers 




C



{\displaystyle \mathbb {C} }

. It is usually denoted using angular brackets by 




⟨

a


,

b

⟩



{\displaystyle \left\langle \mathbf {a} \,,\mathbf {b} \right\rangle }

.
The inner product of two vectors over the field of complex numbers is, in general, a complex number, and is sesquilinear instead of bilinear. An inner product space is a normed vector space, and the inner product of a vector with itself is real and positive-definite.
Functions[edit]
The dot product is defined for vectors that have a finite number of entries. Thus these vectors can be regarded as discrete functions: a length-n vector u is, then, a function with domain {k ∈ ℕ ∣ 1 ≤ k ≤ n}, and ui is a notation for the image of i by the function/vector u.
This notion can be generalized to continuous functions: just as the inner product on vectors uses a sum over corresponding components, the inner product on functions is defined as an integral over some interval a ≤ x ≤ b (also denoted [a, b]):[1]






⟨
u
,
v
⟩

=

∫

a


b


u
(
x
)
v
(
x
)
d
x


{\displaystyle \left\langle u,v\right\rangle =\int _{a}^{b}u(x)v(x)dx}



Generalized further to complex functions ψ(x) and χ(x), by analogy with the complex inner product above, gives[1]






⟨
ψ
,
χ
⟩

=

∫

a


b


ψ
(
x
)



χ
(
x
)

¯


d
x
.


{\displaystyle \left\langle \psi ,\chi \right\rangle =\int _{a}^{b}\psi (x){\overline {\chi (x)}}dx.}



Weight function[edit]
Inner products can have a weight function, i.e. a function which weights each term of the inner product with a value.
Dyadics and matrices[edit]
Matrices have the Frobenius inner product, which is analogous to the vector inner product. It is defined as the sum of the products of the corresponding components of two matrices A and B having the same size:







A


:


B


=

∑

i



∑

j



A

i
j





B

i
j


¯


=

t
r

(


B



H




A

)
=

t
r

(

A



B



H



)
.


{\displaystyle {\mathbf {A}}:{\mathbf {B}}=\sum _{i}\sum _{j}A_{ij}{\overline {B_{ij}}}=\mathrm {tr} (\mathbf {B} ^{\mathrm {H} }\mathbf {A} )=\mathrm {tr} (\mathbf {A} \mathbf {B} ^{\mathrm {H} }).}








A


:


B


=

∑

i



∑

j



A

i
j



B

i
j


=

t
r

(


B



T




A

)
=

t
r

(

A



B



T



)
=

t
r

(


A



T




B

)
=

t
r

(

B



A



T



)
.


{\displaystyle {\mathbf {A}}:{\mathbf {B}}=\sum _{i}\sum _{j}A_{ij}B_{ij}=\mathrm {tr} (\mathbf {B} ^{\mathrm {T} }\mathbf {A} )=\mathrm {tr} (\mathbf {A} \mathbf {B} ^{\mathrm {T} })=\mathrm {tr} (\mathbf {A} ^{\mathrm {T} }\mathbf {B} )=\mathrm {tr} (\mathbf {B} \mathbf {A} ^{\mathrm {T} }).}

 (For real matrices)

Dyadics have a dot product and "double" dot product defined on them, see Dyadics (Product of dyadic and dyadic) for their definitions.
Tensors[edit]
The inner product between a tensor of order n and a tensor of order m is a tensor of order n + m − 2, see tensor contraction for details.
Computation[edit]
Algorithms[edit]
The straightforward algorithm for calculating a floating-point dot product of vectors can suffer from catastrophic cancellation. To avoid this, approaches such as the Kahan summation algorithm are used.
Libraries[edit]
A dot product function is included in BLAS level 1.
See also[edit]

Cauchy–Schwarz inequality
Cross product
Matrix multiplication

Notes[edit]



^ The term scalar product is often also used more generally to mean a symmetric bilinear form, for example for a pseudo-Euclidean space.[citation needed]



References[edit]



^ a b c d e f S. Lipschutz; M. Lipson (2009). Linear Algebra (Schaum’s Outlines) (4th ed.). McGraw Hill. ISBN 978-0-07-154352-1. 
^ a b c M.R. Spiegel; S. Lipschutz; D. Spellman (2009). Vector Analysis (Schaum’s Outlines) (2nd ed.). McGraw Hill. ISBN 978-0-07-161545-7. 
^ A I Borisenko; I E Taparov (1968). Vector and tensor analysis with applications. Translated by Richard Silverman. Dover. p. 14. 
^ Arfken, G. B.; Weber, H. J. (2000). Mathematical Methods for Physicists (5th ed.). Boston, MA: Academic Press. pp. 14–15. ISBN 978-0-12-059825-0. .
^ Weisstein, Eric W. "Dot Product." From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/DotProduct.html
^ T. Banchoff; J. Wermer (1983). Linear Algebra Through Geometry. Springer Science & Business Media. p. 12. ISBN 978-1-4684-0161-5. 
^ A. Bedford; Wallace L. Fowler (2008). Engineering Mechanics: Statics (5th ed.). Prentice Hall. p. 60. ISBN 978-0-13-612915-8. 
^ K.F. Riley; M.P. Hobson; S.J. Bence (2010). Mathematical methods for physics and engineering (3rd ed.). Cambridge University Press. ISBN 978-0-521-86153-3. 
^ M. Mansfield; C. O’Sullivan (2011). Understanding Physics (4th ed.). John Wiley & Sons. ISBN 978-0-47-0746370. 



External links[edit]

Hazewinkel, Michiel, ed. (2001), "Inner product", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W. "Dot product". MathWorld. 


Explanation of dot product including with complex vectors
"Dot Product" by Bruce Torrence, Wolfram Demonstrations Project, 2007.







v
t
e


Linear algebra



Basic concepts



Scalar
Vector
Vector space
Scalar multiplication
Vector projection
Linear span
Linear map
Linear projection
Linear independence
Linear combination
Basis
Column space
Row space
Dual space
Orthogonality
Kernel
Eigenvalues and eigenvectors
Outer product
Inner product space
Dot product
Transpose
Gram–Schmidt process
Linear equations





Vector algebra



Cross product
Triple product
Seven-dimensional cross product





Multilinear algebra



Geometric algebra
Exterior algebra
Bivector
Multivector





Matrices



Block
Decomposition
Invertible
Minor
Multiplication
Rank
Transformation
Cramer's rule
Gaussian elimination





Numerical



Floating point
Matrix Laboratory
Numerical stability
Basic Linear Algebra Subprograms (BLAS)
Sparse matrix
Comparison of linear algebra libraries
Comparison of numerical analysis software












v
t
e


Tensors




Glossary of tensor theory



Scope





Mathematics




coordinate system
multilinear algebra
Euclidean geometry
tensor algebra
dyadic algebra
differential geometry
exterior calculus
tensor calculus









Physics
Engineering







continuum mechanics
electromagnetism
transport phenomena
general relativity
computer vision








Notation



index notation
multi-index notation
Einstein notation
Ricci calculus
Penrose graphical notation
Voigt notation
abstract index notation
tetrad (index notation)
Van der Waerden notation





Tensor definitions



tensor (intrinsic definition)
tensor field
tensor density
tensors in curvilinear coordinates
mixed tensor
antisymmetric tensor
symmetric tensor
tensor operator
tensor bundle





Operations



tensor product
exterior product
tensor contraction
transpose (2nd-order tensors)
raising and lowering indices
Hodge dual
covariant derivative
exterior derivative
exterior covariant derivative
Lie derivative





Related abstractions



dimension
basis
vector, vector space
multivector
covariance and contravariance of vectors
linear transformation
matrix
spinor
Cartan formalism (physics)
differential form
exterior form
connection form
geodesic
manifold
fiber bundle
Levi-Civita connection
affine connection





Notable tensors





Mathematics




Kronecker delta
Levi-Civita symbol
metric tensor
nonmetricity tensor
Christoffel symbols
Ricci curvature
Riemann curvature tensor
Weyl tensor
torsion tensor






Physics




moment of inertia
angular momentum tensor
spin tensor
Cauchy stress tensor
stress–energy tensor
EM tensor
gluon field strength tensor
Einstein tensor
metric tensor (GR)








Mathematicians



Leonhard Euler
Carl Friedrich Gauss
Augustin-Louis Cauchy
Hermann Grassmann
Gregorio Ricci-Curbastro
Tullio Levi-Civita
Jan Arnoldus Schouten
Bernhard Riemann
Elwin Bruno Christoffel
Woldemar Voigt
Élie Cartan
Hermann Weyl
Albert Einstein










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Dot_product&oldid=792018014"					
Categories: Bilinear formsLinear algebraVectors (mathematics and physics)Analytic geometryTensorsHidden categories: All articles with unsourced statementsArticles with unsourced statements from March 2017Wikipedia articles needing clarification from March 2017Articles containing proofs 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


አማርኛالعربيةБеларускаяБългарскиBosanskiCatalàČeštinaDanskDeutschEestiEspañolEsperantoEuskaraفارسیFrançaisGalego한국어Հայերենहिन्दीHrvatskiBahasa IndonesiaItalianoעבריתҚазақшаLatinaLatviešuLietuviųMagyarमराठीBahasa MelayuNederlands日本語Norsk bokmålNorsk nynorskPiemontèisPolskiPortuguêsРусскийScotsSimple EnglishSlovenčinaSlovenščinaکوردیСрпски / srpskiSrpskohrvatski / српскохрватскиSuomiSvenskaTagalogதமிழ்Татарча/tatarçaไทยTürkçeУкраїнськаاردوTiếng Việt中文 
Edit links 





 This page was last edited on 23 July 2017, at 23:08.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 








Vector Products, Inc.: Private Company Information - Bloomberg









































  





















































































August 03, 2017 12:42 PM ET
Electrical Equipment

Company Overview of Vector Products, Inc.



Snapshot People




Company Overview
Vector Products, Inc., doing business as Vector Manufacturing, Ltd., offers power solutions, cordless rechargeable lighting, vehicle direct current accessories, consumer electronics, and garage tools in the United States. It offers various power products, including battery chargers, jump starters, portable power, and power inverters; lighting products, such as spotlights, flashlights, lanterns, worklights, and indoor/outdoor lamps; garage products, including waxer/polishers, sanders, and air inflators; emergency alerts; and other electronic products. The company was incorporated in 2006 and is based in Ft. Lauderdale, Florida. As of March 1, 2006, Vector Products, Inc. operates as a subsidia...
Vector Products, Inc., doing business as Vector Manufacturing, Ltd., offers power solutions, cordless rechargeable lighting, vehicle direct current accessories, consumer electronics, and garage tools in the United States. It offers various power products, including battery chargers, jump starters, portable power, and power inverters; lighting products, such as spotlights, flashlights, lanterns, worklights, and indoor/outdoor lamps; garage products, including waxer/polishers, sanders, and air inflators; emergency alerts; and other electronic products. The company was incorporated in 2006 and is based in Ft. Lauderdale, Florida. As of March 1, 2006, Vector Products, Inc. operates as a subsidiary of Stanley Black & Decker, Inc.
Detailed Description


4140 SW 28th WayFt. Lauderdale, FL 33312United StatesFounded in 2006



Phone: 866-584-5504

Fax: 954-584-5556








Key Executives for Vector Products, Inc.




Mr. David Mayer


      	Chief Executive Officer
      





Compensation as of Fiscal Year 2017. 



Similar Private Companies By Industry



Company Name
Region



 10C Technologies, Inc. United States 24M Technologies, Inc. United States 2D2C, Inc. United States 360 Electrical, LLC United States 4D Energetics Inc. United States




Recent Private Companies Transactions



TypeDate
Target



No transactions available in the past 12 months.




Request Profile Update


				\
	














 












The information and data displayed in this profile are created and managed by S&P Global Market Intelligence, a division of S&P Global. Bloomberg.com does not create or control the content. For inquiries, please contact S&P Global Market Intelligence directly by clicking 
	here.




Stock Quotes


Market data is delayed at least 15 minutes.







Company Lookup



Most Searched Private Companies



Company Name
Geographic Region



 Lawyers Committee for Civil Rights Under Law United States NYC2012, Inc. United States The Advertising Council, Inc. United States Bertelsmann AG Europe Rush University United States













Sponsored Financial Commentaries

Sponsored Links






Browse Companies
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z
 | 
0
1
2
3
4
5
6
7
8
9






Request Profile Update

						Only a company representative may request an update for the company profile. Documentation will be required.
					  
To contact Vector Products, Inc., please visit --.  Company data is provided by S&P Global Market Intelligence.  Please use this form to report any data issues.
					  


Information Missing - Please enter your information in the following field(s): 




Company Name

Your Name*

Your E-Mail Address*

Your Phone Number*

Type of Data*


Overview
Executives
Key developments
Similar companies
Transactions


Update Needed*

All data changes require verification from public sources.  Please include the correct value or values and a source where we can verify.


Cancel


Submit






Your requested update has been submitted
Our data partners will research the update request and update the information on this page if necessary. Research and follow-up could take several weeks. If you have questions, you can contact them at bwwebmaster@businessweek.com.


Close






























Vector - The Freedom of Power
















In March, 2006, Black & Decker purchased Vector Products, Inc.   Since the acquisition, Black & Decker has assumed the manufacturing of all Vector products.  For customer service and warranty information for Vector products, please contact Black & Decker Customer Service.
Black & Decker also manufactures a complete line of automotive products under the Black & Decker brand.  For information about Black & Deckerâ€™s full line, click on the links below:














Black & Decker Home    |   Privacy Policy    |   Terms & Conditions
Copyright 2009 BLACK & DECKER, INC.





Vectron International – Manufacturer of precision oscillators for Communication, Industrial, Military & Space applications

































Search text:






Contact us

|

Login

|




|



















Our company



Overview
Vectron mission
Vectron locations
Knowles




Quality overview
ISO certificates
RoHS/lead-free initiatives
Change Notifications (PCNs)




Contact us

Distributors
Careers




Investor relations
Products



OCXO/EMXO
TCXO
VCSO
VCXO
XO (Crystal Oscillators)
Precision XO
MEMS
Rubidium Standards




Precision Crystals
Standard Crystals
SAW Filters
Crystal and LC Filters
Frequency Translation/Jitter Attenuation
Disciplined Oscillators
GNSS Receivers




1588 Oscillators
Holdover Oscillators
Stratum 3/3E Oscillators
Space & Hi-Rel Oscillators/Filters
MIL 55310 QPL Clocks
High Temp Oscillators
Sensor Products




Low Phase Noise
High Stability
High Shock & Vibration
Low g–sensitivity
Ultra Low Jitter Clocks

Technical Resources
Application Notes
Brochures
Reference Data




Markets

Wireless

Macro Base Stations
Small Cells
Backhaul


Wireline

SONET/SDH/Synchronous Ethernet
IEEE-1588 Version 2
40G / 100G / 400G


Military & Space

Space & Hi-Rel
Radar
Radio Communications
Avionics
Command & Control
Missiles & Precision Guided Munitions


Industrial

GNSS Filters
Test & Measurement
Medical
Data Storage
Energy




Newsroom






































Low phase noise


Highstability


High shock& vibration


Lowg-sensitivity


Extended Temp Range







Download White Paper:High Power SAW Filters


Download Tutorial:Oscillators to Support 1588 Timing






A world leader

							Vectron is a world leader in the design, manufacture and marketing of Frequency Control, Sensor, and Hybrid Product solutions using the very latest techniques in both bulk acoustic wave (BAW) and surface acoustic wave (SAW) based designs from DC to microwave frequencies. 
						
Learn more 





Latest news

08.02.17
Smallest Hi–Rel Clock on the Market Now Available with Leads
07.10.17
Ultra Low Power Real Time Clock for High Temp Applications
06.06.17
VS–800; 3GHz 5x3.2 VCSO Sample and Mass Production Ready

View all 


Advanced Product Search

General Product andReference Design Data Sheet Search











Additional links

Sengenuity Sensor Engines
Knowles
Subscribe to Our Newsletter




Privacy policy | 
				Cookie policy | 
				Terms of service | 
				Contact | 
				Careers |
                Newsletter Subscribe |
                Blog
© Copyright 2017 Vectron




 



Vector Marketing | Our Products | CUTCO® Cutlery















































































Got an Interview?
|
Ready for Training?
 



The Voice
                –  
              Read the Latest Blog Post













To Menu

What We Do
|
Who We Are
 





















Application







People Love Their CUTCO®.
 




                                            People Love Their CUTCO®.                                    


Click To Play


OverlaysPreviousNextPreviousNext


 
High Quality. Guaranteed Forever.

The first CUTCO® knife was produced in Olean, New York in 1949. Since then, we have worked to create a product that people love.
To learn more about CUTCO® visit CUTCO.com.



Apply Now to Work with Vector

























































 



Welcome, Canadian Visitor

Hi. We see that you are visiting our site from Canada.
Vector Marketing Canada has its own website - vectormarketing.ca.
Would you like to visit the Vector Canada site instead of the U.S. one?
Yes, go to Vector Canada
No, stay on Vector U.S.




